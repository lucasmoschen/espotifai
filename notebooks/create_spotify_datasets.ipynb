{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Spotify dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will create music datasets based on a list of Last.fm users, using the Spotipy library which translates Spotify Web API to Python.\n",
    "\n",
    "We need playlists dataset for our analysis, and the easiest way to get it is to search users playlists. But we don't have a list of Spotify users, so we try Last.fm users, because many of them are Spotify users too.\n",
    "\n",
    "## Datasets created\n",
    "\n",
    "- **Users dataset**: a list of Spotify users; from Last.fm users dataset;\n",
    "- **Playlists dataset**: from users dataset;\n",
    "- **Tracks dataset**: from playlists dataset;\n",
    "- **Audio features dataset**: a complement to tracks dataset; from tracks dataset;\n",
    "- **Artists dataset**: from tracks dataset.\n",
    "\n",
    "**P.S.**: The comments through the notebook are intentional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import requests\n",
    "import spotipy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_manager = SpotifyClientCredentials()\n",
    "sp = spotipy.Spotify(auth_manager=auth_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many users do we want to search for playlists?\n",
    "LEN_USERS = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will gather Last.fm users and test if they are Spotify users too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/users.txt') as f:\n",
    "    users = f.read().split('\\n')\n",
    "    \n",
    "random.shuffle(users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test users and obtain playlists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the users and gather their playlists at the same time, if it succeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sp_users = []\n",
    "playlists = []\n",
    "pbar = tqdm(total=LEN_USERS)\n",
    "for user in users:\n",
    "    user_exists = 1\n",
    "    while True:\n",
    "        try:\n",
    "            their_playlists = sp.user_playlists(user)\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        except spotipy.exceptions.SpotifyException:\n",
    "            user_exists = 0\n",
    "        break\n",
    "    if not user_exists:\n",
    "        continue\n",
    "    playlists.extend(their_playlists['items'])\n",
    "    while their_playlists['next']:\n",
    "        while True:\n",
    "            try:\n",
    "                their_playlists = sp.next(their_playlists)\n",
    "            except requests.exceptions.ReadTimeout as e:\n",
    "                print(e)\n",
    "                time.sleep(10)\n",
    "                continue              \n",
    "            break\n",
    "        playlists.extend(their_playlists['items'])\n",
    "    pbar.update()\n",
    "    sp_users.append(user)\n",
    "    if len(sp_users) >= LEN_USERS:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('We have now {} playlists!'.format(len(playlists)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of playlist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "playlists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Spotify users to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/sp_users.txt', 'w') as f:\n",
    "    for user in sp_users:\n",
    "        f.write('{}\\n'.format(user))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: get number of followers info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be necessary to pass again through all playlists just for a bit of information, that is, the number of followers of a playlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i, playlist in tqdm(enumerate(playlists.copy()), total=len(playlists.copy())):\n",
    "#     playlists[i] = sp.playlist(playlists[i]['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat playlists dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we treat the dataset playlists, filtering just what we want. We also expand the `owner` column and remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns\n",
    "playlists = pd.DataFrame(playlists, columns=[\n",
    "    'collaborative',\n",
    "    'description',\n",
    "#     'external_urls',\n",
    "#     'followers',\n",
    "#     'href',\n",
    "    'id',\n",
    "#     'images',\n",
    "    'name',\n",
    "    'owner',\n",
    "    'primary_color',\n",
    "    'public',\n",
    "#     'snapshot_id',\n",
    "    'tracks',\n",
    "#     'type',\n",
    "#     'uri'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand owner dict\n",
    "playlists['owner_id'] = playlists['owner'].apply(pd.Series)['id']\n",
    "playlists.drop(columns='owner', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove duplicates\n",
    "# playlists.drop_duplicates('id')#, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reindex\n",
    "# playlists.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write playlists dataset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "playlists.to_pickle('../data/sp_playlists.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through playlists to get tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now iterate through the playlists dataset in order to gather information about tracks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique the playlist ids\n",
    "playlist_ids = list(set(playlists.id.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration\n",
    "tracks = []\n",
    "for i, playlist_id in tqdm(enumerate(playlist_ids), total=len(playlist_ids)):\n",
    "    while True:\n",
    "        try:\n",
    "            q = sp.playlist_tracks(playlist_id)\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        break\n",
    "    items = q['items'].copy()\n",
    "    for item in items:\n",
    "        # We save the playlist id too\n",
    "        item.update({'playlist_id': playlist_id})\n",
    "    tracks.extend(items)\n",
    "    while q['next']:\n",
    "        while True:\n",
    "            try:\n",
    "                q = sp.next(q)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            break\n",
    "        items = q['items'].copy()\n",
    "        for item in items:\n",
    "            item.update({'playlist_id': playlist_id})\n",
    "        tracks.extend(items)\n",
    "        \n",
    "    # This is necessary because we don't have enough memory\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        pd.DataFrame(tracks).to_pickle('../data/sp_tracks_temp_{}.pkl'.format(i))\n",
    "        tracks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of track:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(tracks) > 0:\n",
    "    tracks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last save of pickles\n",
    "if len(tracks) > 0:\n",
    "    pd.DataFrame(tracks).to_pickle('../data/sp_tracks_temp_{}.pkl'.format(len(playlist_ids)))\n",
    "\n",
    "# Free memory please\n",
    "del tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat tracks database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now treat the tracks database. We do it in all pickle files saved above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(os.path.join('../data/', '*_temp_*.pkl'))\n",
    "for file in tqdm(all_files):\n",
    "    \n",
    "    # Filter\n",
    "    df = pd.read_pickle(file)[[\n",
    "        'added_at',\n",
    "        'added_by',\n",
    "        'is_local',\n",
    "    #     'primary_color',\n",
    "        'track',\n",
    "    #     'video_thumbnail',\n",
    "        'playlist_id',\n",
    "\n",
    "    ]]\n",
    "\n",
    "    # # Drop rows with NaN values\n",
    "    # print('{} rows were dropped.'.format(len(df.drop(df.dropna().index))))\n",
    "    # df.dropna(inplace=True)\n",
    "\n",
    "    # Parse dates\n",
    "    df.added_at = pd.to_datetime(df.added_at)\n",
    "\n",
    "    # Expand added_by column\n",
    "    df['added_by'] = df.added_by.apply(pd.Series).id\n",
    "\n",
    "    # Expand track column\n",
    "    df2 = df.track.apply(pd.Series).copy()\n",
    "    df2 = df2[[\n",
    "        'album',\n",
    "        'artists',\n",
    "        'available_markets',\n",
    "        'disc_number',\n",
    "        'duration_ms',\n",
    "    #     'episode',\n",
    "        'explicit',\n",
    "    #     'external_ids',\n",
    "    #     'external_urls',\n",
    "    #     'href',\n",
    "        'id',\n",
    "    #     'is_local',\n",
    "        'name',\n",
    "        'popularity',\n",
    "    #     'preview_url',\n",
    "    #     'track',\n",
    "        'track_number',\n",
    "    #     'type',\n",
    "    #     'uri',\n",
    "    #     'linked_from'\n",
    "    ]]\n",
    "    df.drop(columns='track', inplace=True)\n",
    "    df = df.join(df2)\n",
    "\n",
    "    # Expand album column\n",
    "    df2 = df.album.apply(pd.Series).copy()\n",
    "    df2 = df2[[\n",
    "        'album_type',\n",
    "        'artists',\n",
    "        'available_markets',\n",
    "    #     'external_urls',\n",
    "    #     'href',\n",
    "        'id',\n",
    "    #     'images',\n",
    "        'name',\n",
    "        'release_date',\n",
    "    #     'release_date_precision',\n",
    "    #     'total_tracks',\n",
    "    #     'type',\n",
    "    #     'uri'\n",
    "    ]]\n",
    "    df2.rename(columns={\n",
    "        'artists': 'album_artists',\n",
    "        'available_markets': 'album_available_markets',\n",
    "        'id': 'album_id',\n",
    "        'name': 'album_name',\n",
    "        'release_date': 'album_release_date'\n",
    "    }, inplace=True)\n",
    "    df.drop(columns='album', inplace=True)\n",
    "    df = df.join(df2)\n",
    "\n",
    "    # Expand artists column\n",
    "    def try_id(d):\n",
    "        try:\n",
    "            ids = [i['id'] for i in d if not pd.isna(i['id'])]\n",
    "            if len(ids) > 0:\n",
    "                return ids\n",
    "        except:\n",
    "            pass\n",
    "        return np.nan\n",
    "    def try_name(d):\n",
    "        try:\n",
    "            ids = [i['name'] for i in d if not pd.isna(i['name'])]\n",
    "            if len(ids) > 0:\n",
    "                return ids\n",
    "        except:\n",
    "            pass\n",
    "        return np.nan\n",
    "    df['artists_ids'] = df.artists.apply(try_id)\n",
    "    df['artists_names'] = df.artists.apply(try_name)\n",
    "    df.drop(columns='artists', inplace=True)\n",
    "\n",
    "    # Expand album_artists column\n",
    "    df['album_artists_ids'] = df.album_artists.apply(try_id)\n",
    "    df['album_artists_names'] = df.album_artists.apply(try_name)\n",
    "    df.drop(columns='album_artists', inplace=True)\n",
    "\n",
    "    # # Drop rows with NaN values\n",
    "    # print('{} rows were dropped.'.format(len(df.drop(df.dropna().index))))\n",
    "    # df.dropna(inplace=True)\n",
    "    \n",
    "    path = file.split('_temp_')\n",
    "    path = path[0] + '_ready_' + path[1]\n",
    "    df.to_pickle(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through tracks to get their features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tracks have features, like `danceability`, which is important for future analysis. We collect the ids of the tracks and search for the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('../data/*_ready_*.pkl')\n",
    "ids = []\n",
    "for file in tqdm(all_files):\n",
    "    ids.extend(list(pd.read_pickle(file).id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [id for id in ids if not pd.isna(id)]\n",
    "ids = list(set(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = []\n",
    "for i in tqdm(range(0, len(ids), 100)):\n",
    "    while True:\n",
    "        try:\n",
    "            q = sp.audio_features(ids[i:i+100])\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        break\n",
    "    audio_features.extend(q)\n",
    "#     if i % 10000 == 0:\n",
    "#         time.sleep(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sp.audio_features` can return `[None]`, so we check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = [track for track in audio_features if not pd.isna(track)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features = pd.DataFrame(audio_features, columns=[\n",
    "    'danceability',\n",
    "    'energy',\n",
    "    'key',\n",
    "    'loudness',\n",
    "    'mode',\n",
    "    'speechiness',\n",
    "    'acousticness',\n",
    "    'instrumentalness',\n",
    "    'liveness',\n",
    "    'valence',\n",
    "    'tempo',\n",
    "#     'type',\n",
    "    'id',\n",
    "#     'uri',\n",
    "#     'track_href',\n",
    "#     'analysis_url',\n",
    "    'duration_ms',\n",
    "    'time_signature'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write audio features dataset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_features.to_pickle('../data/sp_audio_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get artists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to have artists data too, mainly because of track genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('../data/*_ready_*.pkl')\n",
    "artists_ids = []\n",
    "for file in tqdm(all_files):\n",
    "    for item in pd.read_pickle(file).artists_ids:\n",
    "        if type(item) == list:\n",
    "            artists_ids.extend(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_ids = [artists_id for artists_id in artists_ids if not pd.isna(artists_id)]\n",
    "artists_ids = list(set(artists_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = []\n",
    "for i in tqdm(range(0, len(artists_ids), 50)):\n",
    "    while True:\n",
    "        try:\n",
    "            q = sp.artists(artists_ids[i:i+50])\n",
    "        except requests.exceptions.ReadTimeout as e:\n",
    "            print(e)\n",
    "            time.sleep(10)\n",
    "            continue\n",
    "        break\n",
    "    artists.extend(q['artists'])\n",
    "#     if i % 10000 == 0:\n",
    "#         time.sleep(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter columns\n",
    "artists_df = pd.DataFrame(artists, columns=[\n",
    "#     'external_urls',\n",
    "    'followers',\n",
    "    'genres',\n",
    "#     'href',\n",
    "    'id',\n",
    "#     'images',\n",
    "    'name',\n",
    "    'popularity',\n",
    "#     'type',\n",
    "#     'uri'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand followers columns\n",
    "artists_df.followers = artists_df.followers.apply(lambda x: x['total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write artists dataset to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_df.to_pickle('../data/sp_artists.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
