{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview What this is Espotifai is our approach to the playlist continuation problem . That is, given a playlist, how to continuate it? We know nowadays recommenders are extremely important, both for the greater production of content never seen before, and for the massive access provided by digital platforms. Music recommendation is no exception, so it is important to study and develop ways to match people and music they like or they will like. We studied and implemented two algorithms that try to solve the problem of playlist continuation, inspired by Kelen et al. and Pauws and Eggen . Both of them use an idea of k-NN, but the former use a similarity among playlists and the latter use a similarity among tracks. Motivation Recommendation is pretty intrigating, because we have to know by advance what the person would like to listen in a specific time. That's a hard task, due to the human complexity, however we can simplify this problem and we like that! Math is all about that!! And of course, we love music! Our goals Given an incomplete playlist, we should complete it. Also, we should deal with the problem of the leak of playlist data on the internet, so as with the computational problems that appear. This is our final project for Foundations of Data Science , a Mathematical Modelling Master's subject at Getulio Vargas Foundation (FGV).","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#what-this-is","text":"Espotifai is our approach to the playlist continuation problem . That is, given a playlist, how to continuate it? We know nowadays recommenders are extremely important, both for the greater production of content never seen before, and for the massive access provided by digital platforms. Music recommendation is no exception, so it is important to study and develop ways to match people and music they like or they will like. We studied and implemented two algorithms that try to solve the problem of playlist continuation, inspired by Kelen et al. and Pauws and Eggen . Both of them use an idea of k-NN, but the former use a similarity among playlists and the latter use a similarity among tracks.","title":"What this is"},{"location":"#motivation","text":"Recommendation is pretty intrigating, because we have to know by advance what the person would like to listen in a specific time. That's a hard task, due to the human complexity, however we can simplify this problem and we like that! Math is all about that!! And of course, we love music!","title":"Motivation"},{"location":"#our-goals","text":"Given an incomplete playlist, we should complete it. Also, we should deal with the problem of the leak of playlist data on the internet, so as with the computational problems that appear. This is our final project for Foundations of Data Science , a Mathematical Modelling Master's subject at Getulio Vargas Foundation (FGV).","title":"Our goals"},{"location":"conclusion/","text":"Conclusion TO DO","title":"Conclusion"},{"location":"conclusion/#conclusion","text":"TO DO","title":"Conclusion"},{"location":"eda/","text":"EDA TO DO","title":"EDA"},{"location":"eda/#eda","text":"TO DO","title":"EDA"},{"location":"evolution/","text":"Project evolution TO DO","title":"Project evolution"},{"location":"evolution/#project-evolution","text":"TO DO","title":"Project evolution"},{"location":"related_work/","text":"Related work The ACM RecSys Challenge 2018 In 2018, it was organized the ACM RecSys Challenge 2018 , a competition with the goal of playlist continuation. One of the organizators was the Spotify. As described by Chen et al. , it was given the competitors a dataset with one milion Spotify playlists alongside a dataset with incomplete ten thousand playlists to be continuated. The Challenge and the published papers served as inspiration for our work. Papers In 2002, Pauws and Eggen developed PATS: Realization and User Evaluation of an Automatic Playlist Generator , which inspired us in our track-based similairity algorithm. One of the participant groups of the ACM RecSys Challenge 2018, Kelen et al. (2018) inspired us in our playlist-based similarity algorithm. Bonnin and Jannach (2014) gave us an introduction and an overview of the playlist continuation problem. Also was A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures (2003).","title":"Related work"},{"location":"related_work/#related-work","text":"","title":"Related work"},{"location":"related_work/#the-acm-recsys-challenge-2018","text":"In 2018, it was organized the ACM RecSys Challenge 2018 , a competition with the goal of playlist continuation. One of the organizators was the Spotify. As described by Chen et al. , it was given the competitors a dataset with one milion Spotify playlists alongside a dataset with incomplete ten thousand playlists to be continuated. The Challenge and the published papers served as inspiration for our work.","title":"The ACM RecSys Challenge 2018"},{"location":"related_work/#papers","text":"In 2002, Pauws and Eggen developed PATS: Realization and User Evaluation of an Automatic Playlist Generator , which inspired us in our track-based similairity algorithm. One of the participant groups of the ACM RecSys Challenge 2018, Kelen et al. (2018) inspired us in our playlist-based similarity algorithm. Bonnin and Jannach (2014) gave us an introduction and an overview of the playlist continuation problem. Also was A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures (2003).","title":"Papers"},{"location":"model_1/model/","text":"Model of Track's Similarity Matrix Based on the article Steffen Pauws and Berry Eggen The main idea is to establish a metric and build a similarity matrix indicating the probabilitity thet two songs are the same. This will be done using the track's features, track's metadata and the playlists built by the users on Spotify. # Importing libraries import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from scipy.spatial.distance import cdist , squareform , pdist from scipy.sparse import csr_matrix , lil_matrix from sklearn.model_selection import train_test_split from seaborn import heatmap import seaborn as sns import matplotlib.pyplot as plt from tqdm.notebook import tqdm import glob import os Defining Important Features This features will be used to understand the data. We separate the metadata and audio features needed in the process. These features are obtained from Spotify API metadata = [ 'playlist_id' , 'explicit' , 'id' , 'popularity' , 'album_id' , 'album_release_date' , 'artists_ids' ] audio_features = [ 'danceability' , 'energy' , 'loudness' , 'key' , 'mode' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'duration_ms' , 'id' ] Playlist and Tracks dataframes Here we get the playlists, the audio features and the tracks. I separate playlists with more the 5 tracks and less than 500, due to computational problems and considering that people do not make playlists of this size. I will only consider a random part of the dataset, due to computational costs. sample = 1500 # It must be < 10000 playlists_df = pd . read_pickle ( '../../data/sp_playlists.pkl' )[[ 'owner_id' , 'id' , 'tracks' ]] seed = np . random . RandomState ( 100 ) #Reproducibility chosen_users = seed . choice ( playlists_df . owner_id . unique (), size = sample , replace = False ) playlists_df = playlists_df [ playlists_df . owner_id . isin ( chosen_users )] playlists_df . rename ( columns = { 'id' : 'playlist_id' , 'tracks' : 'n_tracks' }, inplace = True ) playlists_df . n_tracks = playlists_df . n_tracks . apply ( lambda x : x [ 'total' ]) # Getting Playlists with at least 5 tracks and maximum of 500 tracks playlists_df = playlists_df [( playlists_df . n_tracks >= 5 ) & ( playlists_df . n_tracks <= 500 )] del playlists_df [ 'n_tracks' ] del playlists_df [ 'owner_id' ] audio_features_df = pd . read_pickle ( '../../data/sp_audio_features.pkl' )[ audio_features ] tracks_df = pd . DataFrame () for file in tqdm ( glob . glob ( '../../data/sp_tracks_ready_*.pkl' )): a = pd . read_pickle ( file )[ metadata ] a = a [ a . playlist_id . isin ( playlists_df . playlist_id )] tracks_df = pd . concat ([ tracks_df , a ], ignore_index = True ) tracks_df = tracks_df . merge ( audio_features_df , on = 'id' ) del audio_features_df del a I will disconsider duplicated songs in the same playlist. I know it may happen, but the algorithm calculates similarity between tracks, and I know it's one. tracks_df = tracks_df . drop_duplicates ([ 'id' , 'playlist_id' ]) Treating the data I convert the dates to datetime and use the year as a continuum value. tracks_df [ 'album_release_date' ] . replace ( to_replace = '0000' , value = None , inplace = True ) tracks_df [ 'album_release_date' ] = pd . to_datetime ( tracks_df [ 'album_release_date' ], errors = 'coerce' ) tracks_df [ 'album_release_date' ] = ( tracks_df [ 'album_release_date' ] - tracks_df [ 'album_release_date' ] . min ()) tracks_df [ 'days' ] = tracks_df [ 'album_release_date' ] / np . timedelta64 ( 1 , 'D' ) We have some few nan values in the column days . I will put the mean of the values, because it's few missing data. It will depend on the initial sample. tracks_df [ 'days' ] . fillna ( np . mean ( tracks_df [ 'days' ]), inplace = True ) Convert the artists to set, in order to the metric presented below. It helps the analysis. tracks_df . artists_ids = tracks_df . artists_ids . apply ( set ) I separate the categorical, numerical and set oriented features, to make the ideia of the similarity matrix. This ideia is withdrawn from article already cited. features_categorical = [ 'explicit' , 'album_id' , 'key' , 'mode' , 'time_signature' ] features_numerical = [ 'popularity' , 'duration_ms' , 'danceability' , 'energy' , 'loudness' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'liveness' , 'valence' , 'tempo' , 'days' ] features_set_oriented = [ 'artists_ids' ] features = [] features . extend ( features_categorical ) features . extend ( features_numerical ) features . extend ( features_set_oriented ) Only to ensure correct type of numerical features. tracks_df [ features_numerical ] = tracks_df [ features_numerical ] . astype ( float ) Defining the metric Let's build the metrics proposed. For now, I normalize the numerical data, ensuring the range to be [0,1] . This ensures the metric works. Consider w a weight vector with size K , the number of features and ||w||_1 = 1 . Consider we have r, s and t features categorical, numerical and set_oriented, respectively, where r + s + t = K . Let x e y be two tracks. So: S = \\sum_{i=1}^r w_i(\\mathbb{1}\\{x_i = y_i\\}) + \\sum_{j = 1}^s w_{r + j}(1 - ||x_j - y_j||_1) + \\sum_{k=1}^t w_{r + s + k} \\frac{|x_k \\cap y_k|}{|x_k \\cup y_k|} scaler = MinMaxScaler () tracks_df [ features_numerical ] = scaler . fit_transform ( tracks_df [ features_numerical ]) I will give grades of importance (1 - 5) based on my experience to each feature. This can be changed, however it will change how the model give importance for each feature in simmilarity calculation. metric_categorical = lambda x1 , x2 : x1 == x2 metric_set_oriented = lambda x1 , x2 : len ( x1 & x2 ) / ( len ( x1 . union ( x2 ))) metric_numerical = lambda x1 , x2 : 1 - abs ( x1 - x2 ) weights = [ 1 , 5 , 2 , 3 , 3 , 2 , 3 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 2 , 2 , 5 ] weights = np . array ( weights ) / sum ( weights ) def metric_songs ( x : np . array , y : np . array ) -> float : similarity = 0 similarity += np . dot ( weights [ 0 : 5 ], metric_categorical ( x [ 0 : 5 ], y [ 0 : 5 ])) similarity += np . dot ( weights [ 5 : 17 ], metric_numerical ( x [ 5 : 17 ], y [ 5 : 17 ])) similarity += weights [ 17 ] * metric_set_oriented ( x [ 17 ], y [ 17 ]) return similarity Simple example Let's calculate a simple case with 500 songs. x1 = np . array ( tracks_df . drop_duplicates ( 'id' )[ features ] . sample ( 500 )) matrix = cdist ( x1 , x1 , metric = metric_songs ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) heatmap ( matrix , ax = ax , cmap = sns . light_palette ( \"green\" )) ax . set_title ( 'Similarity Matrix' ) plt . show () Recommendation based on similarity. We will use the metric described above. The similarity between two songs will be interpreted as a probability . We could build the role track similarity but it requires much computation. So we will do a simple modification. We will calculate the metric between two songs if they are in the same playlist, for some playlist in the dataset. We expect it reduces the number of calculations! After we will have a sparser matrix and in order too add tracks to a playlist, we get n tracks that maximize the mean probability on the similarity matrix, but considering only the tracks on the playlist given. If two tracks appear in the same playlists more times, we use a correction factor. We do as below, if x is the similarity between two tracks, I want f factor that: x \\leq fx < 1 \\implies 1 \\leq f < \\frac{1}{x} I take f as a convex combination of the values. So: f = \\alpha + \\frac{1 - \\alpha}{x} \\implies fx = \\alpha x + (1 - \\alpha), where \\alpha \\in (0,1] . If \\alpha = 1 , we do not have this correction. Evaluation R-precision metric As described in their work, Chen et al. suggests a metric for playlist continuation evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model suggested correctly. A playlist as input to the model has two parts: its part on display to the model and it's hidden part. The hidden part is what the model try to predict and is called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. # Class of the model class SimilarityModel : def __init__ ( self , tracks : pd . DataFrame , playlists : pd . DataFrame ): '''Implementation of the Simmilarity Model described above. The metric used are describe in PATS article. - tracks: all the tracks in your world. - playlists: the training playlists. ''' self . tracks = tracks self . playlists = playlists # We will consider a dataframe with the unique tracks and create numerical indexes self . tracks_index = self . tracks [[ 'id' ]] . drop_duplicates ( 'id' ) . reset_index () self . playlists = self . playlists . set_index ( 'playlist_id' ) def get_similar_track ( self , tracks_similarity : np . array , n_of_songs : int ): '''We get the mean in the tracks similarity and get tracks that maximize the probability mean. - tracks_similarity: matrix with similarities. - n_of_song: the number of songs wanted to be predicted. ''' interest_tracks = tracks_similarity . mean ( axis = 0 ) . A . flatten () songs = np . argpartition ( interest_tracks , - n_of_songs )[ - n_of_songs :] return songs def _get_index ( self , tracks_ids ): indexes = self . tracks_index [ self . tracks_index . id . isin ( tracks_ids )] . index return list ( indexes ) def _get_track_number ( self , index ): track_id = self . tracks_index . loc [ index ] return track_id . id def accuracy_metric ( self , predicted , true ): G_a = set () for artist_id in predicted . artists_ids : G_a = G_a . union ( artist_id ) S_a = set () for artist_id in true . artists_ids : S_a = S_a . union ( artist_id ) G_t = set ( true . id ) S_t = set ( predicted . id ) acc = ( len ( S_t & G_t ) + 0.25 * len ( S_a & G_a )) / len ( G_t ) return acc def fit ( self , alpha = 0.5 ): '''This functions build the model with the tracks and playlists disposed. (1 - alpha) increases the similarity of two tracks if they appear in more playlists. It should be between (0, 1]. ''' assert alpha > 0 assert alpha <= 1 tracks_similarity = lil_matrix (( len ( self . tracks_index ), len ( self . tracks_index )), dtype = float ) for playlist_id in tqdm ( self . playlists . index ): tracks_playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] indexes = self . _get_index ( tracks_playlist . id ) dist = squareform ( pdist ( tracks_playlist , metric = metric_songs )) # M will be a mask. I will multiply it to (fx - x) M = np . heaviside ( tracks_similarity [ np . ix_ ( indexes , indexes )] . A , 0 ) M = M * (( alpha - 1 ) * tracks_similarity [ np . ix_ ( indexes , indexes )] . A + ( 1 - alpha )) M = M + dist tracks_similarity [ np . ix_ ( indexes , indexes )] = M self . tracks_similarity = tracks_similarity . tocsr () def predict ( self , given_tracks : pd . DataFrame , n_of_songs : int ): '''Given a playlist, this function complete it with n_of_songs songs''' n = len ( given_tracks ) indexes = self . _get_index ( given_tracks . id ) similarity = self . tracks_similarity [ indexes ] tracks_chosen = self . get_similar_track ( similarity , n_of_songs ) tracks_id = self . _get_track_number ( tracks_chosen ) predicted_tracks = self . tracks [ self . tracks . id . isin ( tracks_id )] . drop_duplicates ( 'id' ) return predicted_tracks def accuracy_evaluation ( self , playlists : pd . DataFrame = None , rate = 0.7 , bar_show = True ): accuracy = [] if playlists is None : playlists = self . playlists if bar_show : iterator = tqdm ( playlists . index ) else : iterator = playlists . index for playlist_id in iterator : playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] n = len ( playlist ) if n <= 5 : continue # Already known tracks j = int ( rate * n ) if j == 0 : continue playlist_not_hidden = playlist . iloc [ 0 : j ] playlist_hidden = playlist . iloc [ j :] prediction = self . predict ( playlist_not_hidden , n - j ) acc = self . accuracy_metric ( prediction , playlist_hidden ) accuracy . append ( acc ) return np . mean ( accuracy ) Testing the Results First, I will get playlist_id for train and test. I get also only the necessary features from the tracks. I dropped the duplicates cause I'm not interested in playlists with repeated tracks, given that I already know two equal songs have similarity 1. train , test = train_test_split ( playlists_df . drop_duplicates (), test_size = 0.2 , random_state = 412 ) tracks_subset = tracks_df [ features + [ 'id' , 'playlist_id' ]] Let's train the model I will validade the alpha value. It takes a long time to do all the job. Sorry, but you'll have to wait. That's the reason I do not use Cross Validation. It would be better, however. def fitting ( alpha ): print ( 'INFO - Starting with alpha: {} \\n ' . format ( alpha )) model = SimilarityModel ( tracks_subset , training ) model . fit ( alpha = alpha ) acc = model . accuracy_evaluation ( validate , bar_show = False ) evaluation [ alpha ] = acc return acc alphas = [ 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ] training , validate = train_test_split ( train , test_size = 0.2 ) validate = validate . set_index ( 'playlist_id' ) evaluation = dict ( zip ( alphas ,[ 0 , 0 , 0 , 0 , 0 ])) for alpha in alphas : _ = fitting ( alpha ) print ( 'The chosen alpha was {} ' . format ( sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ])) The chosen alpha was (1.0, 0.055581996102373604) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( evaluation . keys (), evaluation . values ()) ax . set_title ( 'Evaluation on the Validation Set' ) ax . set_ylabel ( 'R-precision' ) ax . set_xlabel ( 'alpha' ) plt . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show () So, if two tracks appear together in more thatn a playlist, we don't use this information. Fitting the model with this alpha alpha = sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ][ 0 ] model = SimilarityModel ( tracks_subset , train ) model . fit ( alpha = alpha ) Let's see in the testing ad training set I only have to set test index to playlist_id because it is only done automatically in the training set. test = test . set_index ( 'playlist_id' ) Change the rate of known songs We were considering we already knew 70% tracks of the playlist. I vary this with some values to understang the results. rates = [ 0.2 , 0.5 , 0.7 , 0.9 ] evaluation = { 'Rate' : rates , 'Train Set' : [], 'Test Set' : []} for rate in rates : train_acc = model . accuracy_evaluation ( rate = rate ) test_acc = model . accuracy_evaluation ( test , rate = rate , bar_show = False ) evaluation [ 'Train Set' ] . append ( train_acc ) evaluation [ 'Test Set' ] . append ( test_acc ) evaluation = pd . DataFrame ( evaluation , index = range ( 4 )) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . lineplot ( x = 'Rate' , y = 'Train Set' , data = evaluation , ax = ax [ 0 ]) sns . lineplot ( x = 'Rate' , y = 'Test Set' , data = evaluation , ax = ax [ 1 ], color = 'darkred' ) fig . suptitle ( 'Evaluation on the Test and Train Set' ) ax [ 0 ] . set_title ( 'Train Set' ) ax [ 1 ] . set_title ( 'Test Set' ) ax [ 0 ] . set_ylabel ( 'R-precision' ) ax [ 1 ] . set_ylabel ( 'R-precision' ) ax [ 0 ] . set_xlabel ( 'rate' ) ax [ 1 ] . set_xlabel ( 'rate' ) ax [ 0 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show () Conclusion With low rates, the algorithm performs better. Also, we have to notice that we do not use all the information due to computational cost, but it could improve the results! A well done prefilter could be good to the data, but none thought was good enough.","title":"Model based on track similarity"},{"location":"model_1/model/#model-of-tracks-similarity-matrix","text":"","title":"Model of Track's Similarity Matrix"},{"location":"model_1/model/#based-on-the-article-steffen-pauws-and-berry-eggen","text":"The main idea is to establish a metric and build a similarity matrix indicating the probabilitity thet two songs are the same. This will be done using the track's features, track's metadata and the playlists built by the users on Spotify. # Importing libraries import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from scipy.spatial.distance import cdist , squareform , pdist from scipy.sparse import csr_matrix , lil_matrix from sklearn.model_selection import train_test_split from seaborn import heatmap import seaborn as sns import matplotlib.pyplot as plt from tqdm.notebook import tqdm import glob import os","title":"Based on the article Steffen Pauws and Berry Eggen"},{"location":"model_1/model/#defining-important-features","text":"This features will be used to understand the data. We separate the metadata and audio features needed in the process. These features are obtained from Spotify API metadata = [ 'playlist_id' , 'explicit' , 'id' , 'popularity' , 'album_id' , 'album_release_date' , 'artists_ids' ] audio_features = [ 'danceability' , 'energy' , 'loudness' , 'key' , 'mode' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'duration_ms' , 'id' ]","title":"Defining Important Features"},{"location":"model_1/model/#playlist-and-tracks-dataframes","text":"Here we get the playlists, the audio features and the tracks. I separate playlists with more the 5 tracks and less than 500, due to computational problems and considering that people do not make playlists of this size. I will only consider a random part of the dataset, due to computational costs. sample = 1500 # It must be < 10000 playlists_df = pd . read_pickle ( '../../data/sp_playlists.pkl' )[[ 'owner_id' , 'id' , 'tracks' ]] seed = np . random . RandomState ( 100 ) #Reproducibility chosen_users = seed . choice ( playlists_df . owner_id . unique (), size = sample , replace = False ) playlists_df = playlists_df [ playlists_df . owner_id . isin ( chosen_users )] playlists_df . rename ( columns = { 'id' : 'playlist_id' , 'tracks' : 'n_tracks' }, inplace = True ) playlists_df . n_tracks = playlists_df . n_tracks . apply ( lambda x : x [ 'total' ]) # Getting Playlists with at least 5 tracks and maximum of 500 tracks playlists_df = playlists_df [( playlists_df . n_tracks >= 5 ) & ( playlists_df . n_tracks <= 500 )] del playlists_df [ 'n_tracks' ] del playlists_df [ 'owner_id' ] audio_features_df = pd . read_pickle ( '../../data/sp_audio_features.pkl' )[ audio_features ] tracks_df = pd . DataFrame () for file in tqdm ( glob . glob ( '../../data/sp_tracks_ready_*.pkl' )): a = pd . read_pickle ( file )[ metadata ] a = a [ a . playlist_id . isin ( playlists_df . playlist_id )] tracks_df = pd . concat ([ tracks_df , a ], ignore_index = True ) tracks_df = tracks_df . merge ( audio_features_df , on = 'id' ) del audio_features_df del a I will disconsider duplicated songs in the same playlist. I know it may happen, but the algorithm calculates similarity between tracks, and I know it's one. tracks_df = tracks_df . drop_duplicates ([ 'id' , 'playlist_id' ])","title":"Playlist and Tracks dataframes"},{"location":"model_1/model/#treating-the-data","text":"I convert the dates to datetime and use the year as a continuum value. tracks_df [ 'album_release_date' ] . replace ( to_replace = '0000' , value = None , inplace = True ) tracks_df [ 'album_release_date' ] = pd . to_datetime ( tracks_df [ 'album_release_date' ], errors = 'coerce' ) tracks_df [ 'album_release_date' ] = ( tracks_df [ 'album_release_date' ] - tracks_df [ 'album_release_date' ] . min ()) tracks_df [ 'days' ] = tracks_df [ 'album_release_date' ] / np . timedelta64 ( 1 , 'D' ) We have some few nan values in the column days . I will put the mean of the values, because it's few missing data. It will depend on the initial sample. tracks_df [ 'days' ] . fillna ( np . mean ( tracks_df [ 'days' ]), inplace = True ) Convert the artists to set, in order to the metric presented below. It helps the analysis. tracks_df . artists_ids = tracks_df . artists_ids . apply ( set ) I separate the categorical, numerical and set oriented features, to make the ideia of the similarity matrix. This ideia is withdrawn from article already cited. features_categorical = [ 'explicit' , 'album_id' , 'key' , 'mode' , 'time_signature' ] features_numerical = [ 'popularity' , 'duration_ms' , 'danceability' , 'energy' , 'loudness' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'liveness' , 'valence' , 'tempo' , 'days' ] features_set_oriented = [ 'artists_ids' ] features = [] features . extend ( features_categorical ) features . extend ( features_numerical ) features . extend ( features_set_oriented ) Only to ensure correct type of numerical features. tracks_df [ features_numerical ] = tracks_df [ features_numerical ] . astype ( float )","title":"Treating the data"},{"location":"model_1/model/#defining-the-metric","text":"Let's build the metrics proposed. For now, I normalize the numerical data, ensuring the range to be [0,1] . This ensures the metric works. Consider w a weight vector with size K , the number of features and ||w||_1 = 1 . Consider we have r, s and t features categorical, numerical and set_oriented, respectively, where r + s + t = K . Let x e y be two tracks. So: S = \\sum_{i=1}^r w_i(\\mathbb{1}\\{x_i = y_i\\}) + \\sum_{j = 1}^s w_{r + j}(1 - ||x_j - y_j||_1) + \\sum_{k=1}^t w_{r + s + k} \\frac{|x_k \\cap y_k|}{|x_k \\cup y_k|} scaler = MinMaxScaler () tracks_df [ features_numerical ] = scaler . fit_transform ( tracks_df [ features_numerical ]) I will give grades of importance (1 - 5) based on my experience to each feature. This can be changed, however it will change how the model give importance for each feature in simmilarity calculation. metric_categorical = lambda x1 , x2 : x1 == x2 metric_set_oriented = lambda x1 , x2 : len ( x1 & x2 ) / ( len ( x1 . union ( x2 ))) metric_numerical = lambda x1 , x2 : 1 - abs ( x1 - x2 ) weights = [ 1 , 5 , 2 , 3 , 3 , 2 , 3 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 2 , 2 , 5 ] weights = np . array ( weights ) / sum ( weights ) def metric_songs ( x : np . array , y : np . array ) -> float : similarity = 0 similarity += np . dot ( weights [ 0 : 5 ], metric_categorical ( x [ 0 : 5 ], y [ 0 : 5 ])) similarity += np . dot ( weights [ 5 : 17 ], metric_numerical ( x [ 5 : 17 ], y [ 5 : 17 ])) similarity += weights [ 17 ] * metric_set_oriented ( x [ 17 ], y [ 17 ]) return similarity","title":"Defining the metric"},{"location":"model_1/model/#simple-example","text":"Let's calculate a simple case with 500 songs. x1 = np . array ( tracks_df . drop_duplicates ( 'id' )[ features ] . sample ( 500 )) matrix = cdist ( x1 , x1 , metric = metric_songs ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) heatmap ( matrix , ax = ax , cmap = sns . light_palette ( \"green\" )) ax . set_title ( 'Similarity Matrix' ) plt . show ()","title":"Simple example"},{"location":"model_1/model/#recommendation-based-on-similarity","text":"We will use the metric described above. The similarity between two songs will be interpreted as a probability . We could build the role track similarity but it requires much computation. So we will do a simple modification. We will calculate the metric between two songs if they are in the same playlist, for some playlist in the dataset. We expect it reduces the number of calculations! After we will have a sparser matrix and in order too add tracks to a playlist, we get n tracks that maximize the mean probability on the similarity matrix, but considering only the tracks on the playlist given. If two tracks appear in the same playlists more times, we use a correction factor. We do as below, if x is the similarity between two tracks, I want f factor that: x \\leq fx < 1 \\implies 1 \\leq f < \\frac{1}{x} I take f as a convex combination of the values. So: f = \\alpha + \\frac{1 - \\alpha}{x} \\implies fx = \\alpha x + (1 - \\alpha), where \\alpha \\in (0,1] . If \\alpha = 1 , we do not have this correction.","title":"Recommendation based on similarity."},{"location":"model_1/model/#evaluation","text":"","title":"Evaluation"},{"location":"model_1/model/#r-precision-metric","text":"As described in their work, Chen et al. suggests a metric for playlist continuation evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model suggested correctly. A playlist as input to the model has two parts: its part on display to the model and it's hidden part. The hidden part is what the model try to predict and is called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. # Class of the model class SimilarityModel : def __init__ ( self , tracks : pd . DataFrame , playlists : pd . DataFrame ): '''Implementation of the Simmilarity Model described above. The metric used are describe in PATS article. - tracks: all the tracks in your world. - playlists: the training playlists. ''' self . tracks = tracks self . playlists = playlists # We will consider a dataframe with the unique tracks and create numerical indexes self . tracks_index = self . tracks [[ 'id' ]] . drop_duplicates ( 'id' ) . reset_index () self . playlists = self . playlists . set_index ( 'playlist_id' ) def get_similar_track ( self , tracks_similarity : np . array , n_of_songs : int ): '''We get the mean in the tracks similarity and get tracks that maximize the probability mean. - tracks_similarity: matrix with similarities. - n_of_song: the number of songs wanted to be predicted. ''' interest_tracks = tracks_similarity . mean ( axis = 0 ) . A . flatten () songs = np . argpartition ( interest_tracks , - n_of_songs )[ - n_of_songs :] return songs def _get_index ( self , tracks_ids ): indexes = self . tracks_index [ self . tracks_index . id . isin ( tracks_ids )] . index return list ( indexes ) def _get_track_number ( self , index ): track_id = self . tracks_index . loc [ index ] return track_id . id def accuracy_metric ( self , predicted , true ): G_a = set () for artist_id in predicted . artists_ids : G_a = G_a . union ( artist_id ) S_a = set () for artist_id in true . artists_ids : S_a = S_a . union ( artist_id ) G_t = set ( true . id ) S_t = set ( predicted . id ) acc = ( len ( S_t & G_t ) + 0.25 * len ( S_a & G_a )) / len ( G_t ) return acc def fit ( self , alpha = 0.5 ): '''This functions build the model with the tracks and playlists disposed. (1 - alpha) increases the similarity of two tracks if they appear in more playlists. It should be between (0, 1]. ''' assert alpha > 0 assert alpha <= 1 tracks_similarity = lil_matrix (( len ( self . tracks_index ), len ( self . tracks_index )), dtype = float ) for playlist_id in tqdm ( self . playlists . index ): tracks_playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] indexes = self . _get_index ( tracks_playlist . id ) dist = squareform ( pdist ( tracks_playlist , metric = metric_songs )) # M will be a mask. I will multiply it to (fx - x) M = np . heaviside ( tracks_similarity [ np . ix_ ( indexes , indexes )] . A , 0 ) M = M * (( alpha - 1 ) * tracks_similarity [ np . ix_ ( indexes , indexes )] . A + ( 1 - alpha )) M = M + dist tracks_similarity [ np . ix_ ( indexes , indexes )] = M self . tracks_similarity = tracks_similarity . tocsr () def predict ( self , given_tracks : pd . DataFrame , n_of_songs : int ): '''Given a playlist, this function complete it with n_of_songs songs''' n = len ( given_tracks ) indexes = self . _get_index ( given_tracks . id ) similarity = self . tracks_similarity [ indexes ] tracks_chosen = self . get_similar_track ( similarity , n_of_songs ) tracks_id = self . _get_track_number ( tracks_chosen ) predicted_tracks = self . tracks [ self . tracks . id . isin ( tracks_id )] . drop_duplicates ( 'id' ) return predicted_tracks def accuracy_evaluation ( self , playlists : pd . DataFrame = None , rate = 0.7 , bar_show = True ): accuracy = [] if playlists is None : playlists = self . playlists if bar_show : iterator = tqdm ( playlists . index ) else : iterator = playlists . index for playlist_id in iterator : playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] n = len ( playlist ) if n <= 5 : continue # Already known tracks j = int ( rate * n ) if j == 0 : continue playlist_not_hidden = playlist . iloc [ 0 : j ] playlist_hidden = playlist . iloc [ j :] prediction = self . predict ( playlist_not_hidden , n - j ) acc = self . accuracy_metric ( prediction , playlist_hidden ) accuracy . append ( acc ) return np . mean ( accuracy )","title":"R-precision metric"},{"location":"model_1/model/#testing-the-results","text":"First, I will get playlist_id for train and test. I get also only the necessary features from the tracks. I dropped the duplicates cause I'm not interested in playlists with repeated tracks, given that I already know two equal songs have similarity 1. train , test = train_test_split ( playlists_df . drop_duplicates (), test_size = 0.2 , random_state = 412 ) tracks_subset = tracks_df [ features + [ 'id' , 'playlist_id' ]]","title":"Testing the Results"},{"location":"model_1/model/#lets-train-the-model","text":"I will validade the alpha value. It takes a long time to do all the job. Sorry, but you'll have to wait. That's the reason I do not use Cross Validation. It would be better, however. def fitting ( alpha ): print ( 'INFO - Starting with alpha: {} \\n ' . format ( alpha )) model = SimilarityModel ( tracks_subset , training ) model . fit ( alpha = alpha ) acc = model . accuracy_evaluation ( validate , bar_show = False ) evaluation [ alpha ] = acc return acc alphas = [ 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ] training , validate = train_test_split ( train , test_size = 0.2 ) validate = validate . set_index ( 'playlist_id' ) evaluation = dict ( zip ( alphas ,[ 0 , 0 , 0 , 0 , 0 ])) for alpha in alphas : _ = fitting ( alpha ) print ( 'The chosen alpha was {} ' . format ( sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ])) The chosen alpha was (1.0, 0.055581996102373604) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( evaluation . keys (), evaluation . values ()) ax . set_title ( 'Evaluation on the Validation Set' ) ax . set_ylabel ( 'R-precision' ) ax . set_xlabel ( 'alpha' ) plt . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show () So, if two tracks appear together in more thatn a playlist, we don't use this information. Fitting the model with this alpha alpha = sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ][ 0 ] model = SimilarityModel ( tracks_subset , train ) model . fit ( alpha = alpha )","title":"Let's train the model"},{"location":"model_1/model/#lets-see-in-the-testing-ad-training-set","text":"I only have to set test index to playlist_id because it is only done automatically in the training set. test = test . set_index ( 'playlist_id' )","title":"Let's see in the testing ad training set"},{"location":"model_1/model/#change-the-rate-of-known-songs","text":"We were considering we already knew 70% tracks of the playlist. I vary this with some values to understang the results. rates = [ 0.2 , 0.5 , 0.7 , 0.9 ] evaluation = { 'Rate' : rates , 'Train Set' : [], 'Test Set' : []} for rate in rates : train_acc = model . accuracy_evaluation ( rate = rate ) test_acc = model . accuracy_evaluation ( test , rate = rate , bar_show = False ) evaluation [ 'Train Set' ] . append ( train_acc ) evaluation [ 'Test Set' ] . append ( test_acc ) evaluation = pd . DataFrame ( evaluation , index = range ( 4 )) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . lineplot ( x = 'Rate' , y = 'Train Set' , data = evaluation , ax = ax [ 0 ]) sns . lineplot ( x = 'Rate' , y = 'Test Set' , data = evaluation , ax = ax [ 1 ], color = 'darkred' ) fig . suptitle ( 'Evaluation on the Test and Train Set' ) ax [ 0 ] . set_title ( 'Train Set' ) ax [ 1 ] . set_title ( 'Test Set' ) ax [ 0 ] . set_ylabel ( 'R-precision' ) ax [ 1 ] . set_ylabel ( 'R-precision' ) ax [ 0 ] . set_xlabel ( 'rate' ) ax [ 1 ] . set_xlabel ( 'rate' ) ax [ 0 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show ()","title":"Change the rate of known songs"},{"location":"model_1/model/#conclusion","text":"With low rates, the algorithm performs better. Also, we have to notice that we do not use all the information due to computational cost, but it could improve the results! A well done prefilter could be good to the data, but none thought was good enough.","title":"Conclusion"},{"location":"model_2/model/","text":"Model based on playlist similarity Given a playlist, we want to add more tracks to it: it's the playlist continuation problem. Following Kelen et al. , the idea here is to define a similarity metric between two playlists, select the k most similar playlists to ours, define a score metric for tracks continuing our playlist and choose the best tracks to continue it. from scipy.sparse import lil_matrix from tqdm.notebook import tqdm import glob import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns sns . set () from spotipy.oauth2 import SpotifyClientCredentials import spotipy auth_manager = SpotifyClientCredentials () sp = spotipy . Spotify ( auth_manager = auth_manager ) Treatment of data Tracks Here we load and treat the tracks dataset. def tracks_dfs (): \"\"\"Generator to concatenate the various files.\"\"\" for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' ): df = pd . read_pickle ( file )[[ 'id' , 'playlist_id' , 'artists_ids' ]] yield pd . concat ([ df , pd . DataFrame ({ 'file' : [ file ] * len ( df )})], axis = 1 ) tracks_df = pd . concat ( tqdm ( tracks_dfs (), total = 128 ), ignore_index = True ) tracks_df . dropna ( inplace = True ) # The following is necessary to discard repeated playlists tracks_df [ 'idx' ] = tracks_df . index grouped = tracks_df . groupby ( [ 'playlist_id' , 'file' ] )[ 'idx' ] . apply ( list ) . reset_index () tracks_df = tracks_df . drop ( index = [ el for list in grouped [ grouped . duplicated ( 'playlist_id' )] . idx for el in list ] ) del grouped tracks_df . drop ( columns = 'idx' , inplace = True ) We treat the playlists dataset: playlists = tracks_df . groupby ( 'playlist_id' )[ 'id' ] . apply ( list ) We treat the artists for each track: artists = tracks_df . drop_duplicates ( 'id' ) . set_index ( 'id' ) . artists_ids artists . index . name = 'track_id' artists_ids = dict ( zip ( artists . index , range ( len ( artists )))) Training, validation and test data Now we split the data: def split_data ( playlists , test_frac = 0.2 ): # Only playlists between 5 and 250 tracks query = playlists . apply ( lambda x : len ( x )) playlists = playlists [( query >= 5 ) & ( query <= 250 )] # Split training and test data n_test = int ( np . ceil ( len ( playlists ) * test_frac )) query = playlists . apply ( lambda x : len ( x )) playlists_test = playlists [( query > 25 )] . sample ( n_test ) playlists_training = playlists . drop ( index = playlists_test . index ) return playlists_training , playlists_test playlists_training , playlists_test = split_data ( playlists ) playlists_training , playlists_validation = split_data ( playlists_training ) The model Relevance matrix R We build the relevance matrix R . R_{ij}=r_{ij} indicates if a track j is relevant to the playlist i , that is, the track is in the playlist. Because we will use matrix multiplication, we have to index each track ID and each playlist ID to an index of the matrix. We do it here using dictionaries. def matrix_r ( playlists_training ): \"\"\"Create relevance matrix R.\"\"\" all_tracks = [] for playlist in playlists_training . to_list (): all_tracks . extend ( playlist ) all_tracks = list ( set ( all_tracks )) track_ids_go = dict ( zip ( all_tracks , range ( len ( all_tracks )))) track_ids_back = dict ( zip ( track_ids_go . values (), track_ids_go . keys ())) playlist_ids = dict ( zip ( set ( playlists_training . index ), range ( len ( set ( playlists_training . index ))) )) m = len ( set ( playlists_training . index )) n = len ( set ( all_tracks )) R = lil_matrix (( m , n )) for playlist_id , playlist in playlists_training . iteritems (): for track_id in playlist : R [ playlist_ids [ playlist_id ], track_ids_go [ track_id ]] = 1 return track_ids_go , track_ids_back , playlist_ids , R Similarity The similarity between two playlists u and v is calculated by: s_{uv} = \\sum_{i \\in I} \\dfrac{r_{ui}r_{vi}}{||R_u||_2||R_v||_2} I is the set of tracks and R_u is the vector of relevances r_{ui} for the playlist u . In fact, we basically count the number of tracks in the intersection of the playlists and normalize it. def similarity ( playlist_1 , playlist_2 ): \"\"\"Calculate the similarity between two playlists.\"\"\" summation = len ( set ( playlist_1 ) & set ( playlist_2 )) if summation == 0 : return 0 return summation / np . sqrt ( len ( playlist_1 ) * len ( playlist_2 )) Track score Given a playlist u to be continuated, we calculate the similarity of it with all existent playlists and select the k most similar playlists, that is, the set N_k(u) . So, we define a score for a track to be in the playlist: \\hat{r}_{ui} = \\dfrac{\\sum_{v \\in N_k(u)} s_{uv} \\cdot r_{vi}}{\\sum_{v \\in N_k(u)} s_{uv}} The intuition is that we are giving high scores to tracks that are in many playlists with great similarities to our playlist. We return the tracks ordered by score. def continuation ( R , playlist , playlists_training , k , playlist_ids , track_ids_back ): \"\"\"Continue a playlist based on k most similar playlists.\"\"\" m = len ( set ( playlists_training . index )) s_u = lil_matrix (( 1 , m )) for alt_playlist_index , alt_playlist in playlists_training . items (): s = similarity ( playlist , alt_playlist ) s_u [ 0 , playlist_ids [ alt_playlist_index ]] = s sorted_similarities_indices = np . flip ( np . argsort ( s_u . toarray ()[ 0 ])) top_k_similarities_indices = sorted_similarities_indices [: k ] scores = s_u [ 0 , top_k_similarities_indices ] * R [ top_k_similarities_indices , :] scores = scores . toarray ()[ 0 ] sorted_scores_indices = np . flip ( np . argsort ( scores )[ - 225 :]) return [ track_ids_back [ index ] for index in sorted_scores_indices ] Summary We choose a playlist to be continuated; We calculate the similarity between this and each playlist in the training dataset; We calculate the score of each track continuating our playlist; We choose the tracks with highest score. Evaluation R-precision metric As described in their work, Chen et al. suggest a metric for playlist continuation playlist evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model correctly suggested. A playlist as input to the model has two parts: the part the model will see and the part the model will try to predict, called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. def r_precision ( S_t , G_t , S_a , G_a ): return ( len ( set ( S_t ) & set ( G_t )) + 0.25 * len ( set ( S_a ) & set ( G_a ))) / len ( G_t ) def evaluation ( playlist_not_hidden , playlist_hidden , continuation ): for track in playlist_not_hidden : if track in continuation : continuation . remove ( track ) continuation = continuation [: len ( playlist_hidden )] G_a = [] for track in playlist_hidden : G_a . extend ( artists . iloc [ artists_ids [ track ]]) S_a = [] for track in continuation : S_a . extend ( artists . iloc [ artists_ids [ track ]]) metric = r_precision ( continuation , playlist_hidden , S_a , G_a ) return metric Hyperparameter k We now select a k value that maximizes the R-precision metric in a sample in our validation dataset. It's not feasible to select k by cross-validation, because we need test data to have more than 25 tracks. track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_training ) metrics = [] for k in tqdm ([ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )]): metric_summation = 0 for playlist in tqdm ( playlists_validation . sample ( 1000 )): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric metrics . append ( metric_summation / 1000 ) sns . lineplot ( x = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )], y = metrics , marker = 'o' ) plt . title ( 'R-precision vs. k' ) plt . xlabel ( 'k' ) plt . ylabel ( 'R-precision' ) plt . show () k = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )][ np . argmax ( metrics )] print ( 'The best k is {} .' . format ( k )) The best k is 100. Now we train and evaluate our model: playlists_all_training = pd . concat ([ playlists_training , playlists_validation ]) track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_all_training ) metric_summation = 0 for playlist in tqdm ( playlists_test ): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_all_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric print ( 'R-precision = {:.4f} ' . format ( metric_summation / len ( playlists_test ))) R-precision = 0.1107 As said by Chen et al. , the highest performance achieved in RecSys Challenge 2018 was 0.2241. Well, many competitors were using much more advanced models, like neural networks, and they also had much more data and possible more computational power. So the results we achieved seems much reasonable.","title":"Model based on playlist similarity"},{"location":"model_2/model/#model-based-on-playlist-similarity","text":"Given a playlist, we want to add more tracks to it: it's the playlist continuation problem. Following Kelen et al. , the idea here is to define a similarity metric between two playlists, select the k most similar playlists to ours, define a score metric for tracks continuing our playlist and choose the best tracks to continue it. from scipy.sparse import lil_matrix from tqdm.notebook import tqdm import glob import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns sns . set () from spotipy.oauth2 import SpotifyClientCredentials import spotipy auth_manager = SpotifyClientCredentials () sp = spotipy . Spotify ( auth_manager = auth_manager )","title":"Model based on playlist similarity"},{"location":"model_2/model/#treatment-of-data","text":"","title":"Treatment of data"},{"location":"model_2/model/#tracks","text":"Here we load and treat the tracks dataset. def tracks_dfs (): \"\"\"Generator to concatenate the various files.\"\"\" for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' ): df = pd . read_pickle ( file )[[ 'id' , 'playlist_id' , 'artists_ids' ]] yield pd . concat ([ df , pd . DataFrame ({ 'file' : [ file ] * len ( df )})], axis = 1 ) tracks_df = pd . concat ( tqdm ( tracks_dfs (), total = 128 ), ignore_index = True ) tracks_df . dropna ( inplace = True ) # The following is necessary to discard repeated playlists tracks_df [ 'idx' ] = tracks_df . index grouped = tracks_df . groupby ( [ 'playlist_id' , 'file' ] )[ 'idx' ] . apply ( list ) . reset_index () tracks_df = tracks_df . drop ( index = [ el for list in grouped [ grouped . duplicated ( 'playlist_id' )] . idx for el in list ] ) del grouped tracks_df . drop ( columns = 'idx' , inplace = True ) We treat the playlists dataset: playlists = tracks_df . groupby ( 'playlist_id' )[ 'id' ] . apply ( list ) We treat the artists for each track: artists = tracks_df . drop_duplicates ( 'id' ) . set_index ( 'id' ) . artists_ids artists . index . name = 'track_id' artists_ids = dict ( zip ( artists . index , range ( len ( artists ))))","title":"Tracks"},{"location":"model_2/model/#training-validation-and-test-data","text":"Now we split the data: def split_data ( playlists , test_frac = 0.2 ): # Only playlists between 5 and 250 tracks query = playlists . apply ( lambda x : len ( x )) playlists = playlists [( query >= 5 ) & ( query <= 250 )] # Split training and test data n_test = int ( np . ceil ( len ( playlists ) * test_frac )) query = playlists . apply ( lambda x : len ( x )) playlists_test = playlists [( query > 25 )] . sample ( n_test ) playlists_training = playlists . drop ( index = playlists_test . index ) return playlists_training , playlists_test playlists_training , playlists_test = split_data ( playlists ) playlists_training , playlists_validation = split_data ( playlists_training )","title":"Training, validation and test data"},{"location":"model_2/model/#the-model","text":"","title":"The model"},{"location":"model_2/model/#relevance-matrix-r","text":"We build the relevance matrix R . R_{ij}=r_{ij} indicates if a track j is relevant to the playlist i , that is, the track is in the playlist. Because we will use matrix multiplication, we have to index each track ID and each playlist ID to an index of the matrix. We do it here using dictionaries. def matrix_r ( playlists_training ): \"\"\"Create relevance matrix R.\"\"\" all_tracks = [] for playlist in playlists_training . to_list (): all_tracks . extend ( playlist ) all_tracks = list ( set ( all_tracks )) track_ids_go = dict ( zip ( all_tracks , range ( len ( all_tracks )))) track_ids_back = dict ( zip ( track_ids_go . values (), track_ids_go . keys ())) playlist_ids = dict ( zip ( set ( playlists_training . index ), range ( len ( set ( playlists_training . index ))) )) m = len ( set ( playlists_training . index )) n = len ( set ( all_tracks )) R = lil_matrix (( m , n )) for playlist_id , playlist in playlists_training . iteritems (): for track_id in playlist : R [ playlist_ids [ playlist_id ], track_ids_go [ track_id ]] = 1 return track_ids_go , track_ids_back , playlist_ids , R","title":"Relevance matrix R"},{"location":"model_2/model/#similarity","text":"The similarity between two playlists u and v is calculated by: s_{uv} = \\sum_{i \\in I} \\dfrac{r_{ui}r_{vi}}{||R_u||_2||R_v||_2} I is the set of tracks and R_u is the vector of relevances r_{ui} for the playlist u . In fact, we basically count the number of tracks in the intersection of the playlists and normalize it. def similarity ( playlist_1 , playlist_2 ): \"\"\"Calculate the similarity between two playlists.\"\"\" summation = len ( set ( playlist_1 ) & set ( playlist_2 )) if summation == 0 : return 0 return summation / np . sqrt ( len ( playlist_1 ) * len ( playlist_2 ))","title":"Similarity"},{"location":"model_2/model/#track-score","text":"Given a playlist u to be continuated, we calculate the similarity of it with all existent playlists and select the k most similar playlists, that is, the set N_k(u) . So, we define a score for a track to be in the playlist: \\hat{r}_{ui} = \\dfrac{\\sum_{v \\in N_k(u)} s_{uv} \\cdot r_{vi}}{\\sum_{v \\in N_k(u)} s_{uv}} The intuition is that we are giving high scores to tracks that are in many playlists with great similarities to our playlist. We return the tracks ordered by score. def continuation ( R , playlist , playlists_training , k , playlist_ids , track_ids_back ): \"\"\"Continue a playlist based on k most similar playlists.\"\"\" m = len ( set ( playlists_training . index )) s_u = lil_matrix (( 1 , m )) for alt_playlist_index , alt_playlist in playlists_training . items (): s = similarity ( playlist , alt_playlist ) s_u [ 0 , playlist_ids [ alt_playlist_index ]] = s sorted_similarities_indices = np . flip ( np . argsort ( s_u . toarray ()[ 0 ])) top_k_similarities_indices = sorted_similarities_indices [: k ] scores = s_u [ 0 , top_k_similarities_indices ] * R [ top_k_similarities_indices , :] scores = scores . toarray ()[ 0 ] sorted_scores_indices = np . flip ( np . argsort ( scores )[ - 225 :]) return [ track_ids_back [ index ] for index in sorted_scores_indices ]","title":"Track score"},{"location":"model_2/model/#summary","text":"We choose a playlist to be continuated; We calculate the similarity between this and each playlist in the training dataset; We calculate the score of each track continuating our playlist; We choose the tracks with highest score.","title":"Summary"},{"location":"model_2/model/#evaluation","text":"","title":"Evaluation"},{"location":"model_2/model/#r-precision-metric","text":"As described in their work, Chen et al. suggest a metric for playlist continuation playlist evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model correctly suggested. A playlist as input to the model has two parts: the part the model will see and the part the model will try to predict, called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. def r_precision ( S_t , G_t , S_a , G_a ): return ( len ( set ( S_t ) & set ( G_t )) + 0.25 * len ( set ( S_a ) & set ( G_a ))) / len ( G_t ) def evaluation ( playlist_not_hidden , playlist_hidden , continuation ): for track in playlist_not_hidden : if track in continuation : continuation . remove ( track ) continuation = continuation [: len ( playlist_hidden )] G_a = [] for track in playlist_hidden : G_a . extend ( artists . iloc [ artists_ids [ track ]]) S_a = [] for track in continuation : S_a . extend ( artists . iloc [ artists_ids [ track ]]) metric = r_precision ( continuation , playlist_hidden , S_a , G_a ) return metric","title":"R-precision metric"},{"location":"model_2/model/#hyperparameter-k","text":"We now select a k value that maximizes the R-precision metric in a sample in our validation dataset. It's not feasible to select k by cross-validation, because we need test data to have more than 25 tracks. track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_training ) metrics = [] for k in tqdm ([ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )]): metric_summation = 0 for playlist in tqdm ( playlists_validation . sample ( 1000 )): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric metrics . append ( metric_summation / 1000 ) sns . lineplot ( x = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )], y = metrics , marker = 'o' ) plt . title ( 'R-precision vs. k' ) plt . xlabel ( 'k' ) plt . ylabel ( 'R-precision' ) plt . show () k = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )][ np . argmax ( metrics )] print ( 'The best k is {} .' . format ( k )) The best k is 100. Now we train and evaluate our model: playlists_all_training = pd . concat ([ playlists_training , playlists_validation ]) track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_all_training ) metric_summation = 0 for playlist in tqdm ( playlists_test ): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_all_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric print ( 'R-precision = {:.4f} ' . format ( metric_summation / len ( playlists_test ))) R-precision = 0.1107 As said by Chen et al. , the highest performance achieved in RecSys Challenge 2018 was 0.2241. Well, many competitors were using much more advanced models, like neural networks, and they also had much more data and possible more computational power. So the results we achieved seems much reasonable.","title":"Hyperparameter k"}]}