{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview What this is Espotifai is our approach to the playlist continuation problem . That is, given a playlist, how to continuate it? We know nowadays recommenders are extremely important, both for the greater production of content never seen before, and for the massive access provided by digital platforms. Music recommendation is no exception, so it is important to study and develop ways to match people and music they like or they will like. We studied and implemented two algorithms that try to solve the problem of playlist continuation, inspired by Kelen et al. and Pauws and Eggen . Both of them use an idea of k-NN, but the former use a similarity among playlists and the latter use a similarity among tracks. Motivation Recommendation is pretty intrigating, because we have to know by advance what the person would like to listen in a specific time. That's a hard task, due to the human complexity, however we can simplify this problem and we like that! Math is all about that!! And of course, we love music! Our goals Given an incomplete playlist, we should complete it. Also, we should deal with the problem of the leak of playlist data on the internet, so as with the computational problems that appear. This is our final project for Foundations of Data Science , a Mathematical Modelling Master's subject at Getulio Vargas Foundation (FGV).","title":"Overview"},{"location":"#overview","text":"","title":"Overview"},{"location":"#what-this-is","text":"Espotifai is our approach to the playlist continuation problem . That is, given a playlist, how to continuate it? We know nowadays recommenders are extremely important, both for the greater production of content never seen before, and for the massive access provided by digital platforms. Music recommendation is no exception, so it is important to study and develop ways to match people and music they like or they will like. We studied and implemented two algorithms that try to solve the problem of playlist continuation, inspired by Kelen et al. and Pauws and Eggen . Both of them use an idea of k-NN, but the former use a similarity among playlists and the latter use a similarity among tracks.","title":"What this is"},{"location":"#motivation","text":"Recommendation is pretty intrigating, because we have to know by advance what the person would like to listen in a specific time. That's a hard task, due to the human complexity, however we can simplify this problem and we like that! Math is all about that!! And of course, we love music!","title":"Motivation"},{"location":"#our-goals","text":"Given an incomplete playlist, we should complete it. Also, we should deal with the problem of the leak of playlist data on the internet, so as with the computational problems that appear. This is our final project for Foundations of Data Science , a Mathematical Modelling Master's subject at Getulio Vargas Foundation (FGV).","title":"Our goals"},{"location":"conclusion/","text":"Conclusion In this work we presented two similarity-based methods for automatic playlist continuation. We developed them inspired by the RecSys Challenge 2018 and other works on the topic. The dataset used was generated by the Spotify API and the models were developed using Python . We represented the similarity through a sparse matrix, an efficient data structure for representing a large collection of playlists and tracks, in special we can have multiple lines and columns, as long as many spaces are zero. In the first model, we found that with small start playlists the algorithm outperforms, what is a good result. However this model is not much scalable and some changes must be made, like a prefiltering. In the second model we found that the sparse matrix takes little time to be created and the model acchieved a reasonable performance compared to the models from the RecSys Challenge 2018. Some directions we could follow for future works is combining different algorithms, hybrid algorithms . Other interesting thing to do is to considerer the individual behaviour not directly associated with the playlists made by them, but associated with their psyco, e.g. use the fact the first and last algorithms are more important for the user. We hope you enjoy it!","title":"Conclusion"},{"location":"conclusion/#conclusion","text":"In this work we presented two similarity-based methods for automatic playlist continuation. We developed them inspired by the RecSys Challenge 2018 and other works on the topic. The dataset used was generated by the Spotify API and the models were developed using Python . We represented the similarity through a sparse matrix, an efficient data structure for representing a large collection of playlists and tracks, in special we can have multiple lines and columns, as long as many spaces are zero. In the first model, we found that with small start playlists the algorithm outperforms, what is a good result. However this model is not much scalable and some changes must be made, like a prefiltering. In the second model we found that the sparse matrix takes little time to be created and the model acchieved a reasonable performance compared to the models from the RecSys Challenge 2018. Some directions we could follow for future works is combining different algorithms, hybrid algorithms . Other interesting thing to do is to considerer the individual behaviour not directly associated with the playlists made by them, but associated with their psyco, e.g. use the fact the first and last algorithms are more important for the user. We hope you enjoy it!","title":"Conclusion"},{"location":"evolution/","text":"Project evolution Equally important to the final results is the process we passed through. It was as follows. The dataset problem The most recommended dataset to be used was the Million Playlist Dataset, created by Spotify for the ACM RecSys Challenge 2018 . However, it wasn't available for us to use it. There's no homogeneity in research community about playlist data. We saw that each researcher creates its own dataset. It was difficult to find playlist datasets on internet, and what we found showed us not to be very useful. Solution: dataset creation Last.fm is a social network about music. Using the package Pylast, we gathered data from Last.fm users, in a network process fashion: starting with a few users, we walked through their followers recursively. At the end, we had public information about many users, tracks and artists. Spotify is a music streaming service. Using the package Spotipy and the Spotify Web API we scrapped playlist data from many users. Because Spotify doesn't allow us to request user followers, we couldn't gather the data as with Pylast. So we tested the Last.fm users and we colleted their public playlists if available. At the end, he had many Spotify public playlists, as well as information about the tracks, such as the audio features, and about the artists too. Exploratory data analysis The analysis of the data was an expected step in our project. We did a classic EDA (missing values, distributions etc.) and we also created some interesting visualizations, trying to answer questions like 'what's the genre with most tracks?'. The models Music recommendation models are not so widespread as other data science models, so we had to search for these algorithms. We tested some models, but we found computational problems. In order to solve this, we addapted them and also searched for more efficient algorithms. The two models presented are our approach to solve these problems. We implemented the models, we chose the hyperparamers and we evaluated them. Although both models aren't much advanced, the results we found was good, comparing to top models from ACM RecSys Challenge 2018 . Documentation Documenting our work was also part of the project. We created an GitHub repository to store and version our code, and we documented it. The notebooks are also detailed and explain the models. Finally, we built this website in order to expose our work, using MkDocs site generator, and we produced a screencast.","title":"Project evolution"},{"location":"evolution/#project-evolution","text":"Equally important to the final results is the process we passed through. It was as follows.","title":"Project evolution"},{"location":"evolution/#the-dataset-problem","text":"The most recommended dataset to be used was the Million Playlist Dataset, created by Spotify for the ACM RecSys Challenge 2018 . However, it wasn't available for us to use it. There's no homogeneity in research community about playlist data. We saw that each researcher creates its own dataset. It was difficult to find playlist datasets on internet, and what we found showed us not to be very useful.","title":"The dataset problem"},{"location":"evolution/#solution-dataset-creation","text":"Last.fm is a social network about music. Using the package Pylast, we gathered data from Last.fm users, in a network process fashion: starting with a few users, we walked through their followers recursively. At the end, we had public information about many users, tracks and artists. Spotify is a music streaming service. Using the package Spotipy and the Spotify Web API we scrapped playlist data from many users. Because Spotify doesn't allow us to request user followers, we couldn't gather the data as with Pylast. So we tested the Last.fm users and we colleted their public playlists if available. At the end, he had many Spotify public playlists, as well as information about the tracks, such as the audio features, and about the artists too.","title":"Solution: dataset creation"},{"location":"evolution/#exploratory-data-analysis","text":"The analysis of the data was an expected step in our project. We did a classic EDA (missing values, distributions etc.) and we also created some interesting visualizations, trying to answer questions like 'what's the genre with most tracks?'.","title":"Exploratory data analysis"},{"location":"evolution/#the-models","text":"Music recommendation models are not so widespread as other data science models, so we had to search for these algorithms. We tested some models, but we found computational problems. In order to solve this, we addapted them and also searched for more efficient algorithms. The two models presented are our approach to solve these problems. We implemented the models, we chose the hyperparamers and we evaluated them. Although both models aren't much advanced, the results we found was good, comparing to top models from ACM RecSys Challenge 2018 .","title":"The models"},{"location":"evolution/#documentation","text":"Documenting our work was also part of the project. We created an GitHub repository to store and version our code, and we documented it. The notebooks are also detailed and explain the models. Finally, we built this website in order to expose our work, using MkDocs site generator, and we produced a screencast.","title":"Documentation"},{"location":"related_work/","text":"Related work The ACM RecSys Challenge 2018 In 2018, it was organized the ACM RecSys Challenge 2018 , a competition with the goal of playlist continuation. One of the organizators was the Spotify. As described by Chen et al. , it was given the competitors a dataset with one milion Spotify playlists alongside a dataset with incomplete ten thousand playlists to be continuated. The Challenge and the published papers served as inspiration for our work. Papers In 2002, Pauws and Eggen developed PATS: Realization and User Evaluation of an Automatic Playlist Generator , which inspired us in our track-based similairity algorithm. One of the participant groups of the ACM RecSys Challenge 2018, Kelen et al. (2018) inspired us in our playlist-based similarity algorithm. Bonnin and Jannach (2014) gave us an introduction and an overview of the playlist continuation problem. Also was A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures (2003).","title":"Related work"},{"location":"related_work/#related-work","text":"","title":"Related work"},{"location":"related_work/#the-acm-recsys-challenge-2018","text":"In 2018, it was organized the ACM RecSys Challenge 2018 , a competition with the goal of playlist continuation. One of the organizators was the Spotify. As described by Chen et al. , it was given the competitors a dataset with one milion Spotify playlists alongside a dataset with incomplete ten thousand playlists to be continuated. The Challenge and the published papers served as inspiration for our work.","title":"The ACM RecSys Challenge 2018"},{"location":"related_work/#papers","text":"In 2002, Pauws and Eggen developed PATS: Realization and User Evaluation of an Automatic Playlist Generator , which inspired us in our track-based similairity algorithm. One of the participant groups of the ACM RecSys Challenge 2018, Kelen et al. (2018) inspired us in our playlist-based similarity algorithm. Bonnin and Jannach (2014) gave us an introduction and an overview of the playlist continuation problem. Also was A Large-Scale Evaluation of Acoustic and Subjective Music Similarity Measures (2003).","title":"Papers"},{"location":"eda/eda/","text":"Exploratory Data Analysis We created two datasets: the one from Spotify and the one from Last.fm. The notebooks for dataset creation can be found in their folder in our GitHub repository. The complete Spotify EDA can be viewed here , and the complete Last.fm EDA here . In the following, we briefly describe the EDA process. Spotify dataset analysis Spotify has a web API, and this API has a translation to Python called Spotipy. It allows us to search for tracks, playlists and artists data. Among all the information available, there exists what is called audio features , that is, musical metrics like danceability , loudness and instrumentalness , which could be important for recommendation systems. Data was captured using the API. The approach was: 1. We started with a big number of Last.fm users, because Last.fm allows network search (search for the friends of a user, and its friends, and so on...); 2. We select a subset of 1000 random users which also has a Spotify account; 3. We selected their public playlists; 4. We selected the tracks from these playlists; 5. We selected the audio features and the artists from these tracks. It's important to note that songs don't have genres using this API. Who has genre is the artist of the track. So it's necessary to gather the artists of the tracks in order to have genre information. At the end, we have: Users tracks dataset; List of users ; Playlists of the tracks; Audio features of the tracks; Artists of the tracks. We now make some exploration in order to get known the data and have insights for the recommendation models. from collections import Counter import glob import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns # Necessary to improve exploration pd . set_option ( 'display.max_columns' , None ) # Beautiful Seaborn sns . set () We have many datasets, and we wanna know its variables. Let's sample them. Tracks dataset tracks = pd . read_pickle ( '../../data/sp_tracks_ready_999.pkl' ) tracks . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } added_at added_by is_local playlist_id available_markets ... 15746 2018-06-23 20:54:16+00:00 isab3lla False 3sBVA5SQr1uzHRSSzidYNy [SE] ... Tracks dataset is big enough to make we split it in many files. So we did it. At now we have 11 files just for the tracks. But the other datasets are compressed in a single file each one. list ( tracks . columns ) ['added_at', 'added_by', 'is_local', 'playlist_id', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'id', 'name', 'popularity', 'track_number', 'album_type', 'album_available_markets', 'album_id', 'album_name', 'album_release_date', 'artists_ids', 'artists_names', 'album_artists_ids', 'album_artists_names'] It's important to note that everything on Spotify have an ID. So when we have a song and know the ID of one playlist that contains it, we can use this playlist ID to search for the playlist data. In the dataset, we see that a track have many features, which can be viewed as: Playlist features: information about the playlists, like when the song was added, who added it and the ID of the playlist. Track features: like disc_number , its duration, if its explicit, id , its name, popularity and its number. Album features, including artists of the album; Artists features: their IDs and names. Many features have lists as values. It's because some of them have values which vary in length, like the artists_ids . We don't know if we will have 1, 2 or 32 artists. It will happen with other datasets too. Playlists dataset playlists = pd . read_pickle ( '../../data/sp_playlists.pkl' ) playlists . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } collaborative description id name primary_color ... 915 False 4cbOOTE5HPaKHd7dbgRk8L The Hellacopters Top Hits None ... Here we have basic information about a public playlist. One thing to note is that everything Spotipy returns is a JSON file, in the 'records' format. So in the column tracks we have a dict containing information about the tracks. playlists . loc [ 0 , 'tracks' ] {'href': 'https://api.spotify.com/v1/playlists/0itjZK4e1qZzHL9fNInUJR/tracks', 'total': 63} It's little information, but it's not important. The tracks was gathered from the playlist using the ID of the playlist. Audio features dataset Maybe only track information is not enough to make good recommendation systems. So we gather more information, this time more technician: audio_features = pd . read_pickle ( '../../data/sp_audio_features.pkl' ) audio_features . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } danceability energy key loudness mode speechiness acousticness instrumentalness ... 37769 0.553 0.67 6 -4.601 1 0.0302 0.0213 0.0 ... Here we have the ID of the track and its features, like liveness and speechiness . More information about this technical parte can be found on the documentation of the Web API . Another thing to note is that some of duration_ms differs from the ones in the tracks database. Let's see: my_id = None while pd . isnull ( my_id ): my_id = tracks . sample () . id . iloc [ 0 ] print ( tracks [ tracks . id == my_id ] . duration_ms . iloc [ 0 ], audio_features [ audio_features . id == my_id ] . duration_ms . iloc [ 0 ] ) 248266.0 248267 Artists dataset artists = pd . read_pickle ( '../../data/sp_artists.pkl' ) artists . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } followers genres id name popularity 48244 4560 [australian metal, melodic progressive metal] 1DuzOaU8hyIpzzRQFpAO9b Hemina 21 Artists datasets are simple and autoexplanative. It's important to note genres features, because each cell is a list of genres. As already said, the only way to classify a song in a genre is with the genres of its artists. Visualizations When a track was added to a playlist? data = pd . concat ( [ pd . read_pickle ( file )[ 'added_at' ] for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' )], ignore_index = True ) sns . distplot ( data . dt . year , kde = False ) plt . title ( \"Histogram of added_at\" ) plt . xlabel ( \"Year\" ) plt . show () The max date is: data . max () Timestamp('2042-07-07 10:02:09+0000', tz='UTC') We see some outliers, so we filter them: sns . distplot ( data [( data > '2000' ) & ( data < '2021' )] . dt . year , kde = False , bins = 10 ) plt . title ( \"Histogram of added_at\" ) plt . xlabel ( \"Year\" ) plt . show () Most of the songs were added recently. How much time does a track take? data = pd . concat ( [ pd . read_pickle ( file )[ 'duration_ms' ] for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' )], ignore_index = True ) sns . distplot ( data [ data < 1e6 ], kde = False ) plt . title ( \"Distribution of the duration of the tracks (in ms)\" ) plt . show () The mode of the songs has 3min20s. How danceable, louder, ... are the songs? Audio features dataset has a lot of variables, so we explore some of them. for variable in [ 'danceability' , 'loudness' , 'instrumentalness' , 'tempo' ]: sns . distplot ( audio_features [ variable ], kde = False ) plt . title ( 'Distribution of ' + variable ) if variable == 'loudness' : plt . xlabel ( 'loudness (dB)' ) elif variable == 'tempo' : plt . xlabel ( 'tempo (BPM)' ) plt . show () The features above describe a confidence or a metric made by Spotify. We see that danceability (how danceable a track is) has a very nice distribution, that is, most of songs are quite danceable. Loudness (in dB) concentrates itself around -10dB. The distribution of instrumentalness is curious: there are tracks we may say they are instrumental (around 0.9), there are tracks we would say it's not instrumental (close to, but not equal to, zero), and there are many many songs we are sure they are not instrumental (very close to or equal to zero). Maybe it's because it's easy to say a song is not instrumental (it's easy to recognize voice), but the reverse is not so true, that is, to ensure the song is instrumental. temp is another variable with a quite nice distribution. Songs vary it's tempo (in BPM) around 120. One thing to say is that these graphs are quite compatible with those from Spotify's Web API . It suggests that we are in the right way with our data. How popular are the artists? sns . distplot ( artists . popularity , kde = False ) plt . title ( 'Distribution of artists popularity' ) plt . show () This distribution is very close to the distribution of the popularity of the songs, curiously. Finnaly, what are the genres with more tracks? # We will only sample data because the datasets are big # This cell takes a while to run data = pd . concat ( [ pd . read_pickle ( file )[ 'artists_ids' ] . sample ( 1000 ) for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' )], ignore_index = True ) artist_ids = [] for item in data . to_list (): if type ( item ) == list : for artist_id in item : artist_ids . append ( artist_id ) genres = [] for id in artist_ids : q = artists [ artists . id == id ] . genres . iloc [ 0 ] if type ( q ) == list : genres . extend ( q ) counter = Counter ( genres ) ax = sns . barplot ( list ( list ( zip ( * counter . most_common ()))[ 0 ][: 15 ]), [ i / len ( genres ) for i in list ( zip ( * counter . most_common ()))[ 1 ]][: 15 ] ) plt . xticks ( rotation = 90 ) plt . title ( \"Genres with more tracks\" ) # https://stackoverflow.com/a/31357733 vals = ax . get_yticks () ax . set_yticklabels ([ ' {:,.2%} ' . format ( x ) for x in vals ]) plt . show () Again, that's pretty reasonable. Pop and rock was expected to be in this list, so as rap and hip hop. Again, maybe the percentages seem small, but we have a lot of them (only in the sample): len ( set ( genres )) 2297 Many of the genres as sub-sub-subgenres, as you can see: [ genres [ np . random . randint ( len ( genres ))] for _ in range ( 15 )] ['piseiro', 'latin rock', 'alternative dance', 'instrumental rock', 'baile pop', 'indie soul', 'rock', 'swedish synthpop', 'singer-songwriter', 'alternative hip hop', 'skate punk', 'shimmer pop', 'dance pop', 'hip hop', 'german underground rap'] Are there correlations between numerical variables? Let's analyse correlation in audio features dataset. sns . heatmap ( audio_features . corr ()) plt . title ( \"Correlation between audio features\" ) plt . show () We see strong correlations between acousticness and energy : they have strong negative correlation, because acoustic tracks in general are more calm; acousticness and loudness : the same as the latter; energy and loudness : the more energy the song, the louder it is; valence and danceability : the more valence (positiveness), the more danceable it is. instrumental and loudness : we found that a instrumental song tends to be less louder. Last.fm Dataset Analysis The considered datasets are: Users : information like gender, when he/her has registered, country, top artists, top tracks, etc. Tracks : information about reaching, playcounts, similar tracks, top tags, etc. Artists : information about number of listeners, top tracks, top albums, etc. Tags : information about registration, taggings, reaching, etc. This tags is done by user. FOLDER_PATH = '../../data/lastfm-api/' user_info_path = FOLDER_PATH + '1k_users_info_lastfm.json' track_info_path = FOLDER_PATH + 'tracks_lastfm_info.json' artist_info_path = FOLDER_PATH + 'artists_lastfm_info.json' tag_info_path = FOLDER_PATH + 'tags_lastfm_info.json' def get_random_users ( filepath : str , quantity : int = 1000 , random_state : int = 200 ) -> pd . DataFrame : users = pd . read_csv ( filepath , index_col = 'user_id' ) chosen_users = users . sample ( n = quantity , replace = False , random_state = random_state , axis = 'index' ) chosen_users . index = list ( range ( 0 , len ( chosen_users ))) return chosen_users users_df = pd . read_csv ( FOLDER_PATH + 'users_lastfm.csv' , index_col = 'user_id' ) track_df = pd . read_csv ( FOLDER_PATH + 'tracks.csv' , sep = ' \\t ' , index_col = 'track_id' ) artist_df = pd . read_csv ( FOLDER_PATH + 'artists.csv' , sep = ' \\t ' , index_col = 'artist_id' ) tag_df = pd . read_csv ( FOLDER_PATH + 'tags.csv' , sep = ' \\t ' , index_col = 'tag_id' ) User Dataset The information are name, subscriber, playcount, registered_since, country, age, playlists, gender, loved_tracks, recent_tracks, top_tracks, top_tags, top_albums e top_artists. I observe all the users considered don't insert age information neither gender. Create playlists in Last.fm is not a common thing too! I get the unique values in the three columns How many subscribers and how is the refistration distribution? fig , ax = plt . subplots ( 1 , 3 , figsize = ( 24 , 4.5 )) ax [ 2 ] . pie ( users_complete_df [ 'subscriber' ] . value_counts (), labels = [ 'No' , 'Yes' ], labeldistance = None , autopct = ' %1.1f%% ' , textprops = { 'fontsize' : 15 }) ax [ 2 ] . set_title ( 'Is subscriber?' , fontsize = 13 ) ax [ 2 ] . legend () years = [ date . year for date in users_complete_df . registered_since ] months = [ date . month for date in users_complete_df . registered_since ] sns . distplot ( years , kde = False , bins = max ( years ) - min ( years ) + 1 , norm_hist = True , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Year' , fontsize = 13 ) sns . distplot ( months , kde = False , bins = 12 , norm_hist = True , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Month' , fontsize = 13 ) ax [ 1 ] . xaxis . set_major_locator ( ticker . FixedLocator ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ])) ax [ 1 ] . xaxis . set_major_formatter ( ticker . FixedFormatter ([ 'Jan' , 'Feb' , 'Mar' , 'Apr' , 'May' , 'Jun' , 'Jul' , 'Aug' , 'Sep' , 'Oct' , 'Nov' , 'Dec' ])) fig . suptitle ( 'When did the users register?' , fontsize = 15 ) fig . savefig ( '../images/subscriber-registration.png' ) plt . show () In the following graphics: For each x \\mapsto f(x) if x people have listened to more than f(x) tracks! fig , ax = plt . subplots ( figsize = ( 9 , 5 )) ax . set ( xscale = 'log' , yscale = 'log' , xlim = ( 1 , 1000 ), ylim = ( 1 , 6000000 )) sns . scatterplot ( x = range ( 1 , len ( users_complete_df ) + 1 ), y = users_complete_df . playcount . sort_values ( ascending = False ), s = 20 , color = 'black' ) ax . set_title ( 'Playcounts' , fontsize = 14 ) ax . set_xlabel ( 'number of people' ) plt . show () We can see the distribution of the countries in the sample. Brazil, USA and United Kingdom has clear advantage. fig , ax = plt . subplots ( figsize = ( 17 , 4 )) countries_df = users_complete_df . country . value_counts ( True )[ users_complete_df . country . value_counts () . values > 2 ] sns . barplot ( x = countries_df . index , y = countries_df . values , ax = ax ) ax . set_xticklabels ( labels = ax . get_xticklabels (), rotation = 60 ) ax . set_title ( 'Users Countries' , fontsize = 20 ) ax . set_xlabel ( 'Countries' ) plt . show () Considering the top tags for the users, we can get the 10 tags more used in general (weighted by the number of counts) and the 10 tags more used by individuals (no weight). top_tags_count_df = pd . DataFrame ({ 'tag_id' : list ( top_tags_count_weight . keys ()), 'weight_user' : list ( top_tags_count_weight . values ()), 'users_listeners' : list ( top_tags_count . values ())} ) tags = tag_df . merge ( top_tags_count_df , on = 'tag_id' ) display ( tags . sort_values ( by = 'weight_user' , ascending = False ) . head ( 10 )) display ( tags . sort_values ( by = 'users_listeners' , ascending = False ) . head ( 10 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_id tag weight_user users_listeners 187 188 All 52112 2 217 218 spotify 45867 2 647 648 katarakt 4351 1 552 553 essentials 2836 1 648 649 scare the kids 2138 1 94 95 albums I own 1960 5 48 49 pop 1892 86 47 48 noise 1544 18 450 451 heavy metal 1502 9 649 650 ponyhof 1472 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_id tag weight_user users_listeners 48 49 pop 1892 86 39 40 indie 667 72 49 50 rock 1425 72 67 68 electronic 845 67 50 51 alternative 753 61 105 106 experimental 889 48 59 60 female vocalists 1225 38 64 65 alternative rock 527 36 0 1 Hip-Hop 248 33 6 7 folk 283 32 Tag Dataset The information are: name, reached, taggings, published, toptracks, topartists e topalbums We can see the tags topped by tagging by the user. reached = { int ( key ): tags_info [ key ][ 'reached' ] for key in tags_info . keys () if len ( tags_info [ key ]) > 0 } tagging = { int ( key ): tags_info [ key ][ 'taggings' ] for key in tags_info . keys () if len ( tags_info [ key ]) > 0 } published = { int ( key ): tags_info [ key ][ 'published' ] for key in tags_info . keys () if len ( tags_info [ key ]) > 0 } tags_extra_info = pd . DataFrame ({ 'tag_id' : list ( reached . keys ()), 'tag_reached' : list ( reached . values ()), 'taggings' : list ( tagging . values ()), 'published' : list ( published . values ())}) tags_complete_df = tag_df . merge ( tags_extra_info , on = 'tag_id' ) tags_complete_df . sort_values ( by = 'taggings' , ascending = False , inplace = True ) tags_complete_df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_id tag tag_reached taggings published 49 50 rock 395583 3979593 None 67 68 electronic 254123 2371259 None 73 74 seen live 81737 2142301 None 50 51 alternative 261864 2095454 None 39 40 indie 253561 2017410 None Artist Dataset The information are: name, listeners, plays, published, topalbums, toptags, toptracks e similar fig , ax = plt . subplots ( 1 , 3 , figsize = ( 18 , 4.5 )) sns . distplot ( artists_complete_df . year , kde = False , bins = int ( artists_complete_df . year . max () - artists_complete_df . year . min ()) + 1 , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Published Year on Last.fm' , fontsize = 13 ) sns . distplot ( artists_complete_df . listeners , kde = False , #bins = int(artists_complete_df.year.max() - artists_complete_df.year.min()) + 1, ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Distribution of Listeners on Last.fm' , fontsize = 13 ) sns . distplot ( artists_complete_df . plays , kde = False , #bins = int(artists_complete_df.year.max() - artists_complete_df.year.min()) + 1, ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'Distribution of Plays' , fontsize = 13 ) fig . suptitle ( 'Artist Information' , fontsize = 15 ) fig . savefig ( '../../images/artist_info.png' ) plt . show () Artists Similarity Last.fm API has information about similar artists, given an artist imputed. I generate 20 similar artists from each artist of the subset of artists known from the dataset. The method of the API returns a degree of similarity, from 0 to 1. Below we can see de result. It takes long to make this graphic, so I save it. You can see the result. There are a lot of nan values because we do not have every degree of similary. We can see there are a bigger relation in the roundness of the diagonal. This happens because the way the artist id was generated. For each user, we get its 20 top artists and numerate if the id does not exist. So, if two ids are closely, maybe it was generated by the same user, what is interesting, cause users may like similar artists. Subset of Artists Similary Here we can see a subset of the matrix of artists' similarity. It's really sparse, as expected, because we get only 20 similar artists for each one. Track Dataset The information are: name, artist, duration, listeners, playcount, album, published, top_tags e similar info = [] for key in tracks_info . keys (): if len ( tracks_info [ key ]) == 0 : info . append ([]) continue d = tracks_info [ key ][ 'duration' ] l = tracks_info [ key ][ 'listeners' ] p = tracks_info [ key ][ 'playcount' ] pu = tracks_info [ key ][ 'published' ] tags = tracks_info [ key ][ 'top_tags' ] info . append ([ int ( key ), int ( d ), int ( l ), int ( p ), pu ]) info_df = pd . DataFrame ( info , columns = [ 'track_id' , 'duration' , 'listeners' , 'playcount' , 'published' ]) tracks_complete_df = track_df . merge ( info_df , on = 'track_id' ) tracks_complete_df [ 'year' ] = tracks_complete_df [ 'published' ] . apply ( lambda x : int ( x [ 0 : 4 ]) if x else None ) tracks_complete_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track_id artist_name track_name duration listeners playcount published year 4608 4733 Godflesh Streetcleaner 402000.0 30692.0 114054.0 None NaN We can see that published information is unavailable for a lot of tracks Baseline model The ideia is simple and uses the models alreadt done by Last.fm. We believe is a good start, cause they have more information about the users. We'll get the artists from the tracks and top artists from the users. After that we will take the intersection, take the top 10 artists and take their top tracks. After we'll classify and get the 10 top songs to indicate! # Getting important variables FOLDER_SPOTIFY_PATH = '../../data/' tracks_sp_df = pd . read_pickle ( FOLDER_SPOTIFY_PATH + 'sp_tracks_ready_*.pkl' ) tracks_sp_df = tracks_sp_df [[ 'name' , 'artists_names' , 'playlist_id' ]] playlist_df = pd . read_pickle ( FOLDER_SPOTIFY_PATH + 'sp_playlists.pkl' ) playlist_df = playlist_df [[ 'id' , 'owner_id' ]] API_LAST_KEY = input () API_LAST_SECRET = input () network = pylast . LastFMNetwork ( API_LAST_KEY , API_LAST_SECRET ) playlists = tracks_sp_df . playlist_id . unique ()[ 0 : 10 ] def song_indication ( playlist_id : str , user_id : str , number_of_songs : int ): tracks = tracks_sp_df [ tracks_sp_df . playlist_id == playlist_id ] artists_set = {} user_artists_set = {} original_tracks = np . array ( tracks [[ 'name' , 'artists_names' ]]) for index , info in tracks . iterrows (): for artist in info . artists_names : artist = network . get_artist ( artist ) if artist . name . lower () in artists_set : artists_set [ artist . name . lower ()] += 1 else : artists_set [ artist . name . lower ()] = 1 artists_set = { key : artists_set [ key ] / sum ( artists_set . values ()) for key in artists_set . keys ()} user = network . get_user ( user_id ) top_artists = user . get_top_artists ( limit = 200 ) for top_artist in top_artists : artist_name = top_artist . item . name weight = top_artist . weight user_artists_set [ artist_name . lower ()] = int ( weight ) uses_artists_set = { key : user_artists_set [ key ] / sum ( user_artists_set . values ()) for key in user_artists_set . keys ()} chosen_artists = { key : artists_set [ key ] * user_artists_set [ key ] for key in ( artists_set . keys () & user_artists_set . keys ())} if len ( chosen_artists ) == 0 : raise Exception ( 'Sorry, this method does not work' ) # Take 10 at the limit! chosen_artists = sorted ( chosen_artists . items (), key = lambda x : x [ 1 ], reverse = True )[: 10 ] top_tracks = [] for artist_name in chosen_artists : artist = network . get_artist ( artist_name [ 0 ]) tracks = artist . get_top_tracks ( limit = 10 ) for track in tracks : top_tracks . append (( artist_name [ 0 ], track . item . title , track . weight )) chosen_tracks = sorted ( top_tracks , key = lambda x : x [ 2 ], reverse = True )[: number_of_songs ] chosen_tracks = [( i [ 1 ], i [ 0 ]) for i in chosen_tracks ] return chosen_tracks , original_tracks position = 20 indications , original = song_indication ( playlist_df [ 'id' ] . loc [ position ], playlist_df [ 'owner_id' ] . iloc [ position ], 5 ) print ( 'The indications are: ' ) for ind in indications : print ( ind ) print ( '' ) print ( 'The original were: ' ) for ori in original : print ( ori ) The indications are: ('Tempo Perdido', 'legi\u00e3o urbana') ('Pais e Filhos', 'legi\u00e3o urbana') ('\u00cdndios', 'legi\u00e3o urbana') ('Ser\u00e1', 'legi\u00e3o urbana') ('Faroeste Caboclo', 'legi\u00e3o urbana') The original were: ['S\u00e9timo C\u00e9u - Ao Vivo' list(['Geraldo Azevedo'])] ['Respeita januario' list(['Luiz Gonzaga'])] ['N\u00e3o Pegue Esse Avi\u00e3o' list(['Cavaleiros do Forr\u00f3'])] ['Xote das Meninas' list(['Luiz Gonzaga'])] ['Apenas Mais uma de Amor' list(['Lulu Santos'])] ['100% Voc\u00ea' list(['Chiclete Com Banana'])] ['Lanterna dos Afogados - Ao Vivo' list(['Maria Gad\u00fa'])] ['A Vida N\u00e3o T\u00e1 F\u00e1cil Pr\u00e1 Ningu\u00e9m (Sony Music Live)' list(['Jota Quest'])] ['Tempo Perdido' list(['Legi\u00e3o Urbana'])]","title":"EDA"},{"location":"eda/eda/#exploratory-data-analysis","text":"We created two datasets: the one from Spotify and the one from Last.fm. The notebooks for dataset creation can be found in their folder in our GitHub repository. The complete Spotify EDA can be viewed here , and the complete Last.fm EDA here . In the following, we briefly describe the EDA process.","title":"Exploratory Data Analysis"},{"location":"eda/eda/#spotify-dataset-analysis","text":"Spotify has a web API, and this API has a translation to Python called Spotipy. It allows us to search for tracks, playlists and artists data. Among all the information available, there exists what is called audio features , that is, musical metrics like danceability , loudness and instrumentalness , which could be important for recommendation systems. Data was captured using the API. The approach was: 1. We started with a big number of Last.fm users, because Last.fm allows network search (search for the friends of a user, and its friends, and so on...); 2. We select a subset of 1000 random users which also has a Spotify account; 3. We selected their public playlists; 4. We selected the tracks from these playlists; 5. We selected the audio features and the artists from these tracks. It's important to note that songs don't have genres using this API. Who has genre is the artist of the track. So it's necessary to gather the artists of the tracks in order to have genre information. At the end, we have: Users tracks dataset; List of users ; Playlists of the tracks; Audio features of the tracks; Artists of the tracks. We now make some exploration in order to get known the data and have insights for the recommendation models. from collections import Counter import glob import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns # Necessary to improve exploration pd . set_option ( 'display.max_columns' , None ) # Beautiful Seaborn sns . set () We have many datasets, and we wanna know its variables. Let's sample them.","title":"Spotify dataset analysis"},{"location":"eda/eda/#tracks-dataset","text":"tracks = pd . read_pickle ( '../../data/sp_tracks_ready_999.pkl' ) tracks . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } added_at added_by is_local playlist_id available_markets ... 15746 2018-06-23 20:54:16+00:00 isab3lla False 3sBVA5SQr1uzHRSSzidYNy [SE] ... Tracks dataset is big enough to make we split it in many files. So we did it. At now we have 11 files just for the tracks. But the other datasets are compressed in a single file each one. list ( tracks . columns ) ['added_at', 'added_by', 'is_local', 'playlist_id', 'available_markets', 'disc_number', 'duration_ms', 'explicit', 'id', 'name', 'popularity', 'track_number', 'album_type', 'album_available_markets', 'album_id', 'album_name', 'album_release_date', 'artists_ids', 'artists_names', 'album_artists_ids', 'album_artists_names'] It's important to note that everything on Spotify have an ID. So when we have a song and know the ID of one playlist that contains it, we can use this playlist ID to search for the playlist data. In the dataset, we see that a track have many features, which can be viewed as: Playlist features: information about the playlists, like when the song was added, who added it and the ID of the playlist. Track features: like disc_number , its duration, if its explicit, id , its name, popularity and its number. Album features, including artists of the album; Artists features: their IDs and names. Many features have lists as values. It's because some of them have values which vary in length, like the artists_ids . We don't know if we will have 1, 2 or 32 artists. It will happen with other datasets too.","title":"Tracks dataset"},{"location":"eda/eda/#playlists-dataset","text":"playlists = pd . read_pickle ( '../../data/sp_playlists.pkl' ) playlists . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } collaborative description id name primary_color ... 915 False 4cbOOTE5HPaKHd7dbgRk8L The Hellacopters Top Hits None ... Here we have basic information about a public playlist. One thing to note is that everything Spotipy returns is a JSON file, in the 'records' format. So in the column tracks we have a dict containing information about the tracks. playlists . loc [ 0 , 'tracks' ] {'href': 'https://api.spotify.com/v1/playlists/0itjZK4e1qZzHL9fNInUJR/tracks', 'total': 63} It's little information, but it's not important. The tracks was gathered from the playlist using the ID of the playlist.","title":"Playlists dataset"},{"location":"eda/eda/#audio-features-dataset","text":"Maybe only track information is not enough to make good recommendation systems. So we gather more information, this time more technician: audio_features = pd . read_pickle ( '../../data/sp_audio_features.pkl' ) audio_features . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } danceability energy key loudness mode speechiness acousticness instrumentalness ... 37769 0.553 0.67 6 -4.601 1 0.0302 0.0213 0.0 ... Here we have the ID of the track and its features, like liveness and speechiness . More information about this technical parte can be found on the documentation of the Web API . Another thing to note is that some of duration_ms differs from the ones in the tracks database. Let's see: my_id = None while pd . isnull ( my_id ): my_id = tracks . sample () . id . iloc [ 0 ] print ( tracks [ tracks . id == my_id ] . duration_ms . iloc [ 0 ], audio_features [ audio_features . id == my_id ] . duration_ms . iloc [ 0 ] ) 248266.0 248267","title":"Audio features dataset"},{"location":"eda/eda/#artists-dataset","text":"artists = pd . read_pickle ( '../../data/sp_artists.pkl' ) artists . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } followers genres id name popularity 48244 4560 [australian metal, melodic progressive metal] 1DuzOaU8hyIpzzRQFpAO9b Hemina 21 Artists datasets are simple and autoexplanative. It's important to note genres features, because each cell is a list of genres. As already said, the only way to classify a song in a genre is with the genres of its artists.","title":"Artists dataset"},{"location":"eda/eda/#visualizations","text":"When a track was added to a playlist? data = pd . concat ( [ pd . read_pickle ( file )[ 'added_at' ] for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' )], ignore_index = True ) sns . distplot ( data . dt . year , kde = False ) plt . title ( \"Histogram of added_at\" ) plt . xlabel ( \"Year\" ) plt . show () The max date is: data . max () Timestamp('2042-07-07 10:02:09+0000', tz='UTC') We see some outliers, so we filter them: sns . distplot ( data [( data > '2000' ) & ( data < '2021' )] . dt . year , kde = False , bins = 10 ) plt . title ( \"Histogram of added_at\" ) plt . xlabel ( \"Year\" ) plt . show () Most of the songs were added recently. How much time does a track take? data = pd . concat ( [ pd . read_pickle ( file )[ 'duration_ms' ] for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' )], ignore_index = True ) sns . distplot ( data [ data < 1e6 ], kde = False ) plt . title ( \"Distribution of the duration of the tracks (in ms)\" ) plt . show () The mode of the songs has 3min20s. How danceable, louder, ... are the songs? Audio features dataset has a lot of variables, so we explore some of them. for variable in [ 'danceability' , 'loudness' , 'instrumentalness' , 'tempo' ]: sns . distplot ( audio_features [ variable ], kde = False ) plt . title ( 'Distribution of ' + variable ) if variable == 'loudness' : plt . xlabel ( 'loudness (dB)' ) elif variable == 'tempo' : plt . xlabel ( 'tempo (BPM)' ) plt . show () The features above describe a confidence or a metric made by Spotify. We see that danceability (how danceable a track is) has a very nice distribution, that is, most of songs are quite danceable. Loudness (in dB) concentrates itself around -10dB. The distribution of instrumentalness is curious: there are tracks we may say they are instrumental (around 0.9), there are tracks we would say it's not instrumental (close to, but not equal to, zero), and there are many many songs we are sure they are not instrumental (very close to or equal to zero). Maybe it's because it's easy to say a song is not instrumental (it's easy to recognize voice), but the reverse is not so true, that is, to ensure the song is instrumental. temp is another variable with a quite nice distribution. Songs vary it's tempo (in BPM) around 120. One thing to say is that these graphs are quite compatible with those from Spotify's Web API . It suggests that we are in the right way with our data. How popular are the artists? sns . distplot ( artists . popularity , kde = False ) plt . title ( 'Distribution of artists popularity' ) plt . show () This distribution is very close to the distribution of the popularity of the songs, curiously. Finnaly, what are the genres with more tracks? # We will only sample data because the datasets are big # This cell takes a while to run data = pd . concat ( [ pd . read_pickle ( file )[ 'artists_ids' ] . sample ( 1000 ) for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' )], ignore_index = True ) artist_ids = [] for item in data . to_list (): if type ( item ) == list : for artist_id in item : artist_ids . append ( artist_id ) genres = [] for id in artist_ids : q = artists [ artists . id == id ] . genres . iloc [ 0 ] if type ( q ) == list : genres . extend ( q ) counter = Counter ( genres ) ax = sns . barplot ( list ( list ( zip ( * counter . most_common ()))[ 0 ][: 15 ]), [ i / len ( genres ) for i in list ( zip ( * counter . most_common ()))[ 1 ]][: 15 ] ) plt . xticks ( rotation = 90 ) plt . title ( \"Genres with more tracks\" ) # https://stackoverflow.com/a/31357733 vals = ax . get_yticks () ax . set_yticklabels ([ ' {:,.2%} ' . format ( x ) for x in vals ]) plt . show () Again, that's pretty reasonable. Pop and rock was expected to be in this list, so as rap and hip hop. Again, maybe the percentages seem small, but we have a lot of them (only in the sample): len ( set ( genres )) 2297 Many of the genres as sub-sub-subgenres, as you can see: [ genres [ np . random . randint ( len ( genres ))] for _ in range ( 15 )] ['piseiro', 'latin rock', 'alternative dance', 'instrumental rock', 'baile pop', 'indie soul', 'rock', 'swedish synthpop', 'singer-songwriter', 'alternative hip hop', 'skate punk', 'shimmer pop', 'dance pop', 'hip hop', 'german underground rap'] Are there correlations between numerical variables? Let's analyse correlation in audio features dataset. sns . heatmap ( audio_features . corr ()) plt . title ( \"Correlation between audio features\" ) plt . show () We see strong correlations between acousticness and energy : they have strong negative correlation, because acoustic tracks in general are more calm; acousticness and loudness : the same as the latter; energy and loudness : the more energy the song, the louder it is; valence and danceability : the more valence (positiveness), the more danceable it is. instrumental and loudness : we found that a instrumental song tends to be less louder.","title":"Visualizations"},{"location":"eda/eda/#lastfm-dataset-analysis","text":"The considered datasets are: Users : information like gender, when he/her has registered, country, top artists, top tracks, etc. Tracks : information about reaching, playcounts, similar tracks, top tags, etc. Artists : information about number of listeners, top tracks, top albums, etc. Tags : information about registration, taggings, reaching, etc. This tags is done by user. FOLDER_PATH = '../../data/lastfm-api/' user_info_path = FOLDER_PATH + '1k_users_info_lastfm.json' track_info_path = FOLDER_PATH + 'tracks_lastfm_info.json' artist_info_path = FOLDER_PATH + 'artists_lastfm_info.json' tag_info_path = FOLDER_PATH + 'tags_lastfm_info.json' def get_random_users ( filepath : str , quantity : int = 1000 , random_state : int = 200 ) -> pd . DataFrame : users = pd . read_csv ( filepath , index_col = 'user_id' ) chosen_users = users . sample ( n = quantity , replace = False , random_state = random_state , axis = 'index' ) chosen_users . index = list ( range ( 0 , len ( chosen_users ))) return chosen_users users_df = pd . read_csv ( FOLDER_PATH + 'users_lastfm.csv' , index_col = 'user_id' ) track_df = pd . read_csv ( FOLDER_PATH + 'tracks.csv' , sep = ' \\t ' , index_col = 'track_id' ) artist_df = pd . read_csv ( FOLDER_PATH + 'artists.csv' , sep = ' \\t ' , index_col = 'artist_id' ) tag_df = pd . read_csv ( FOLDER_PATH + 'tags.csv' , sep = ' \\t ' , index_col = 'tag_id' )","title":"Last.fm Dataset Analysis"},{"location":"eda/eda/#user-dataset","text":"The information are name, subscriber, playcount, registered_since, country, age, playlists, gender, loved_tracks, recent_tracks, top_tracks, top_tags, top_albums e top_artists. I observe all the users considered don't insert age information neither gender. Create playlists in Last.fm is not a common thing too! I get the unique values in the three columns How many subscribers and how is the refistration distribution? fig , ax = plt . subplots ( 1 , 3 , figsize = ( 24 , 4.5 )) ax [ 2 ] . pie ( users_complete_df [ 'subscriber' ] . value_counts (), labels = [ 'No' , 'Yes' ], labeldistance = None , autopct = ' %1.1f%% ' , textprops = { 'fontsize' : 15 }) ax [ 2 ] . set_title ( 'Is subscriber?' , fontsize = 13 ) ax [ 2 ] . legend () years = [ date . year for date in users_complete_df . registered_since ] months = [ date . month for date in users_complete_df . registered_since ] sns . distplot ( years , kde = False , bins = max ( years ) - min ( years ) + 1 , norm_hist = True , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Year' , fontsize = 13 ) sns . distplot ( months , kde = False , bins = 12 , norm_hist = True , ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Month' , fontsize = 13 ) ax [ 1 ] . xaxis . set_major_locator ( ticker . FixedLocator ([ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 ])) ax [ 1 ] . xaxis . set_major_formatter ( ticker . FixedFormatter ([ 'Jan' , 'Feb' , 'Mar' , 'Apr' , 'May' , 'Jun' , 'Jul' , 'Aug' , 'Sep' , 'Oct' , 'Nov' , 'Dec' ])) fig . suptitle ( 'When did the users register?' , fontsize = 15 ) fig . savefig ( '../images/subscriber-registration.png' ) plt . show () In the following graphics: For each x \\mapsto f(x) if x people have listened to more than f(x) tracks! fig , ax = plt . subplots ( figsize = ( 9 , 5 )) ax . set ( xscale = 'log' , yscale = 'log' , xlim = ( 1 , 1000 ), ylim = ( 1 , 6000000 )) sns . scatterplot ( x = range ( 1 , len ( users_complete_df ) + 1 ), y = users_complete_df . playcount . sort_values ( ascending = False ), s = 20 , color = 'black' ) ax . set_title ( 'Playcounts' , fontsize = 14 ) ax . set_xlabel ( 'number of people' ) plt . show () We can see the distribution of the countries in the sample. Brazil, USA and United Kingdom has clear advantage. fig , ax = plt . subplots ( figsize = ( 17 , 4 )) countries_df = users_complete_df . country . value_counts ( True )[ users_complete_df . country . value_counts () . values > 2 ] sns . barplot ( x = countries_df . index , y = countries_df . values , ax = ax ) ax . set_xticklabels ( labels = ax . get_xticklabels (), rotation = 60 ) ax . set_title ( 'Users Countries' , fontsize = 20 ) ax . set_xlabel ( 'Countries' ) plt . show () Considering the top tags for the users, we can get the 10 tags more used in general (weighted by the number of counts) and the 10 tags more used by individuals (no weight). top_tags_count_df = pd . DataFrame ({ 'tag_id' : list ( top_tags_count_weight . keys ()), 'weight_user' : list ( top_tags_count_weight . values ()), 'users_listeners' : list ( top_tags_count . values ())} ) tags = tag_df . merge ( top_tags_count_df , on = 'tag_id' ) display ( tags . sort_values ( by = 'weight_user' , ascending = False ) . head ( 10 )) display ( tags . sort_values ( by = 'users_listeners' , ascending = False ) . head ( 10 )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_id tag weight_user users_listeners 187 188 All 52112 2 217 218 spotify 45867 2 647 648 katarakt 4351 1 552 553 essentials 2836 1 648 649 scare the kids 2138 1 94 95 albums I own 1960 5 48 49 pop 1892 86 47 48 noise 1544 18 450 451 heavy metal 1502 9 649 650 ponyhof 1472 1 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_id tag weight_user users_listeners 48 49 pop 1892 86 39 40 indie 667 72 49 50 rock 1425 72 67 68 electronic 845 67 50 51 alternative 753 61 105 106 experimental 889 48 59 60 female vocalists 1225 38 64 65 alternative rock 527 36 0 1 Hip-Hop 248 33 6 7 folk 283 32","title":"User Dataset"},{"location":"eda/eda/#tag-dataset","text":"The information are: name, reached, taggings, published, toptracks, topartists e topalbums We can see the tags topped by tagging by the user. reached = { int ( key ): tags_info [ key ][ 'reached' ] for key in tags_info . keys () if len ( tags_info [ key ]) > 0 } tagging = { int ( key ): tags_info [ key ][ 'taggings' ] for key in tags_info . keys () if len ( tags_info [ key ]) > 0 } published = { int ( key ): tags_info [ key ][ 'published' ] for key in tags_info . keys () if len ( tags_info [ key ]) > 0 } tags_extra_info = pd . DataFrame ({ 'tag_id' : list ( reached . keys ()), 'tag_reached' : list ( reached . values ()), 'taggings' : list ( tagging . values ()), 'published' : list ( published . values ())}) tags_complete_df = tag_df . merge ( tags_extra_info , on = 'tag_id' ) tags_complete_df . sort_values ( by = 'taggings' , ascending = False , inplace = True ) tags_complete_df . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } tag_id tag tag_reached taggings published 49 50 rock 395583 3979593 None 67 68 electronic 254123 2371259 None 73 74 seen live 81737 2142301 None 50 51 alternative 261864 2095454 None 39 40 indie 253561 2017410 None","title":"Tag Dataset"},{"location":"eda/eda/#artist-dataset","text":"The information are: name, listeners, plays, published, topalbums, toptags, toptracks e similar fig , ax = plt . subplots ( 1 , 3 , figsize = ( 18 , 4.5 )) sns . distplot ( artists_complete_df . year , kde = False , bins = int ( artists_complete_df . year . max () - artists_complete_df . year . min ()) + 1 , ax = ax [ 0 ]) ax [ 0 ] . set_title ( 'Published Year on Last.fm' , fontsize = 13 ) sns . distplot ( artists_complete_df . listeners , kde = False , #bins = int(artists_complete_df.year.max() - artists_complete_df.year.min()) + 1, ax = ax [ 1 ]) ax [ 1 ] . set_title ( 'Distribution of Listeners on Last.fm' , fontsize = 13 ) sns . distplot ( artists_complete_df . plays , kde = False , #bins = int(artists_complete_df.year.max() - artists_complete_df.year.min()) + 1, ax = ax [ 2 ]) ax [ 2 ] . set_title ( 'Distribution of Plays' , fontsize = 13 ) fig . suptitle ( 'Artist Information' , fontsize = 15 ) fig . savefig ( '../../images/artist_info.png' ) plt . show ()","title":"Artist Dataset"},{"location":"eda/eda/#artists-similarity","text":"Last.fm API has information about similar artists, given an artist imputed. I generate 20 similar artists from each artist of the subset of artists known from the dataset. The method of the API returns a degree of similarity, from 0 to 1. Below we can see de result. It takes long to make this graphic, so I save it. You can see the result. There are a lot of nan values because we do not have every degree of similary. We can see there are a bigger relation in the roundness of the diagonal. This happens because the way the artist id was generated. For each user, we get its 20 top artists and numerate if the id does not exist. So, if two ids are closely, maybe it was generated by the same user, what is interesting, cause users may like similar artists. Subset of Artists Similary Here we can see a subset of the matrix of artists' similarity. It's really sparse, as expected, because we get only 20 similar artists for each one.","title":"Artists Similarity"},{"location":"eda/eda/#track-dataset","text":"The information are: name, artist, duration, listeners, playcount, album, published, top_tags e similar info = [] for key in tracks_info . keys (): if len ( tracks_info [ key ]) == 0 : info . append ([]) continue d = tracks_info [ key ][ 'duration' ] l = tracks_info [ key ][ 'listeners' ] p = tracks_info [ key ][ 'playcount' ] pu = tracks_info [ key ][ 'published' ] tags = tracks_info [ key ][ 'top_tags' ] info . append ([ int ( key ), int ( d ), int ( l ), int ( p ), pu ]) info_df = pd . DataFrame ( info , columns = [ 'track_id' , 'duration' , 'listeners' , 'playcount' , 'published' ]) tracks_complete_df = track_df . merge ( info_df , on = 'track_id' ) tracks_complete_df [ 'year' ] = tracks_complete_df [ 'published' ] . apply ( lambda x : int ( x [ 0 : 4 ]) if x else None ) tracks_complete_df . sample () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } track_id artist_name track_name duration listeners playcount published year 4608 4733 Godflesh Streetcleaner 402000.0 30692.0 114054.0 None NaN We can see that published information is unavailable for a lot of tracks","title":"Track Dataset"},{"location":"eda/eda/#baseline-model","text":"The ideia is simple and uses the models alreadt done by Last.fm. We believe is a good start, cause they have more information about the users. We'll get the artists from the tracks and top artists from the users. After that we will take the intersection, take the top 10 artists and take their top tracks. After we'll classify and get the 10 top songs to indicate! # Getting important variables FOLDER_SPOTIFY_PATH = '../../data/' tracks_sp_df = pd . read_pickle ( FOLDER_SPOTIFY_PATH + 'sp_tracks_ready_*.pkl' ) tracks_sp_df = tracks_sp_df [[ 'name' , 'artists_names' , 'playlist_id' ]] playlist_df = pd . read_pickle ( FOLDER_SPOTIFY_PATH + 'sp_playlists.pkl' ) playlist_df = playlist_df [[ 'id' , 'owner_id' ]] API_LAST_KEY = input () API_LAST_SECRET = input () network = pylast . LastFMNetwork ( API_LAST_KEY , API_LAST_SECRET ) playlists = tracks_sp_df . playlist_id . unique ()[ 0 : 10 ] def song_indication ( playlist_id : str , user_id : str , number_of_songs : int ): tracks = tracks_sp_df [ tracks_sp_df . playlist_id == playlist_id ] artists_set = {} user_artists_set = {} original_tracks = np . array ( tracks [[ 'name' , 'artists_names' ]]) for index , info in tracks . iterrows (): for artist in info . artists_names : artist = network . get_artist ( artist ) if artist . name . lower () in artists_set : artists_set [ artist . name . lower ()] += 1 else : artists_set [ artist . name . lower ()] = 1 artists_set = { key : artists_set [ key ] / sum ( artists_set . values ()) for key in artists_set . keys ()} user = network . get_user ( user_id ) top_artists = user . get_top_artists ( limit = 200 ) for top_artist in top_artists : artist_name = top_artist . item . name weight = top_artist . weight user_artists_set [ artist_name . lower ()] = int ( weight ) uses_artists_set = { key : user_artists_set [ key ] / sum ( user_artists_set . values ()) for key in user_artists_set . keys ()} chosen_artists = { key : artists_set [ key ] * user_artists_set [ key ] for key in ( artists_set . keys () & user_artists_set . keys ())} if len ( chosen_artists ) == 0 : raise Exception ( 'Sorry, this method does not work' ) # Take 10 at the limit! chosen_artists = sorted ( chosen_artists . items (), key = lambda x : x [ 1 ], reverse = True )[: 10 ] top_tracks = [] for artist_name in chosen_artists : artist = network . get_artist ( artist_name [ 0 ]) tracks = artist . get_top_tracks ( limit = 10 ) for track in tracks : top_tracks . append (( artist_name [ 0 ], track . item . title , track . weight )) chosen_tracks = sorted ( top_tracks , key = lambda x : x [ 2 ], reverse = True )[: number_of_songs ] chosen_tracks = [( i [ 1 ], i [ 0 ]) for i in chosen_tracks ] return chosen_tracks , original_tracks position = 20 indications , original = song_indication ( playlist_df [ 'id' ] . loc [ position ], playlist_df [ 'owner_id' ] . iloc [ position ], 5 ) print ( 'The indications are: ' ) for ind in indications : print ( ind ) print ( '' ) print ( 'The original were: ' ) for ori in original : print ( ori ) The indications are: ('Tempo Perdido', 'legi\u00e3o urbana') ('Pais e Filhos', 'legi\u00e3o urbana') ('\u00cdndios', 'legi\u00e3o urbana') ('Ser\u00e1', 'legi\u00e3o urbana') ('Faroeste Caboclo', 'legi\u00e3o urbana') The original were: ['S\u00e9timo C\u00e9u - Ao Vivo' list(['Geraldo Azevedo'])] ['Respeita januario' list(['Luiz Gonzaga'])] ['N\u00e3o Pegue Esse Avi\u00e3o' list(['Cavaleiros do Forr\u00f3'])] ['Xote das Meninas' list(['Luiz Gonzaga'])] ['Apenas Mais uma de Amor' list(['Lulu Santos'])] ['100% Voc\u00ea' list(['Chiclete Com Banana'])] ['Lanterna dos Afogados - Ao Vivo' list(['Maria Gad\u00fa'])] ['A Vida N\u00e3o T\u00e1 F\u00e1cil Pr\u00e1 Ningu\u00e9m (Sony Music Live)' list(['Jota Quest'])] ['Tempo Perdido' list(['Legi\u00e3o Urbana'])]","title":"Baseline model"},{"location":"model_1/model/","text":"Model of Track's Similarity Matrix Based on the article Steffen Pauws and Berry Eggen The main idea is to establish a metric and build a similarity matrix indicating the probabilitity thet two songs are the same. This will be done using the track's features, track's metadata and the playlists built by the users on Spotify. # Importing libraries import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from scipy.spatial.distance import cdist , squareform , pdist from scipy.sparse import csr_matrix , lil_matrix from sklearn.model_selection import train_test_split from seaborn import heatmap import seaborn as sns import matplotlib.pyplot as plt from tqdm.notebook import tqdm import glob import os Defining Important Features This features will be used to understand the data. We separate the metadata and audio features needed in the process. These features are obtained from Spotify API metadata = [ 'playlist_id' , 'explicit' , 'id' , 'popularity' , 'album_id' , 'album_release_date' , 'artists_ids' ] audio_features = [ 'danceability' , 'energy' , 'loudness' , 'key' , 'mode' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'duration_ms' , 'id' ] Playlist and Tracks dataframes Here we get the playlists, the audio features and the tracks. I separate playlists with more the 5 tracks and less than 500, due to computational problems and considering that people do not make playlists of this size. I will only consider a random part of the dataset, due to computational costs. sample = 1500 # It must be < 10000 playlists_df = pd . read_pickle ( '../../data/sp_playlists.pkl' )[[ 'owner_id' , 'id' , 'tracks' ]] seed = np . random . RandomState ( 100 ) #Reproducibility chosen_users = seed . choice ( playlists_df . owner_id . unique (), size = sample , replace = False ) playlists_df = playlists_df [ playlists_df . owner_id . isin ( chosen_users )] playlists_df . rename ( columns = { 'id' : 'playlist_id' , 'tracks' : 'n_tracks' }, inplace = True ) playlists_df . n_tracks = playlists_df . n_tracks . apply ( lambda x : x [ 'total' ]) # Getting Playlists with at least 5 tracks and maximum of 500 tracks playlists_df = playlists_df [( playlists_df . n_tracks >= 5 ) & ( playlists_df . n_tracks <= 500 )] del playlists_df [ 'n_tracks' ] del playlists_df [ 'owner_id' ] audio_features_df = pd . read_pickle ( '../../data/sp_audio_features.pkl' )[ audio_features ] tracks_df = pd . DataFrame () for file in tqdm ( glob . glob ( '../../data/sp_tracks_ready_*.pkl' )): a = pd . read_pickle ( file )[ metadata ] a = a [ a . playlist_id . isin ( playlists_df . playlist_id )] tracks_df = pd . concat ([ tracks_df , a ], ignore_index = True ) tracks_df = tracks_df . merge ( audio_features_df , on = 'id' ) del audio_features_df del a I will disconsider duplicated songs in the same playlist. I know it may happen, but the algorithm calculates similarity between tracks, and I know it's one. tracks_df = tracks_df . drop_duplicates ([ 'id' , 'playlist_id' ]) Treating the data I convert the dates to datetime and use the year as a continuum value. tracks_df [ 'album_release_date' ] . replace ( to_replace = '0000' , value = None , inplace = True ) tracks_df [ 'album_release_date' ] = pd . to_datetime ( tracks_df [ 'album_release_date' ], errors = 'coerce' ) tracks_df [ 'album_release_date' ] = ( tracks_df [ 'album_release_date' ] - tracks_df [ 'album_release_date' ] . min ()) tracks_df [ 'days' ] = tracks_df [ 'album_release_date' ] / np . timedelta64 ( 1 , 'D' ) We have some few nan values in the column days . I will put the mean of the values, because it's few missing data. It will depend on the initial sample. tracks_df [ 'days' ] . fillna ( np . mean ( tracks_df [ 'days' ]), inplace = True ) Convert the artists to set, in order to the metric presented below. It helps the analysis. tracks_df . artists_ids = tracks_df . artists_ids . apply ( set ) I separate the categorical, numerical and set oriented features, to make the ideia of the similarity matrix. This ideia is withdrawn from article already cited. features_categorical = [ 'explicit' , 'album_id' , 'key' , 'mode' , 'time_signature' ] features_numerical = [ 'popularity' , 'duration_ms' , 'danceability' , 'energy' , 'loudness' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'liveness' , 'valence' , 'tempo' , 'days' ] features_set_oriented = [ 'artists_ids' ] features = [] features . extend ( features_categorical ) features . extend ( features_numerical ) features . extend ( features_set_oriented ) Only to ensure correct type of numerical features. tracks_df [ features_numerical ] = tracks_df [ features_numerical ] . astype ( float ) Defining the metric Let's build the metrics proposed. For now, I normalize the numerical data, ensuring the range to be [0,1] . This ensures the metric works. Consider w a weight vector with size K , the number of features and ||w||_1 = 1 . Consider we have r, s and t features categorical, numerical and set_oriented, respectively, where r + s + t = K . Let x e y be two tracks. So: S = \\sum_{i=1}^r w_i(\\mathbb{1}\\{x_i = y_i\\}) + \\sum_{j = 1}^s w_{r + j}(1 - ||x_j - y_j||_1) + \\sum_{k=1}^t w_{r + s + k} \\frac{|x_k \\cap y_k|}{|x_k \\cup y_k|} scaler = MinMaxScaler () tracks_df [ features_numerical ] = scaler . fit_transform ( tracks_df [ features_numerical ]) I will give grades of importance (1 - 5) based on my experience to each feature. This can be changed, however it will change how the model give importance for each feature in simmilarity calculation. metric_categorical = lambda x1 , x2 : x1 == x2 metric_set_oriented = lambda x1 , x2 : len ( x1 & x2 ) / ( len ( x1 . union ( x2 ))) metric_numerical = lambda x1 , x2 : 1 - abs ( x1 - x2 ) weights = [ 1 , 5 , 2 , 3 , 3 , 2 , 3 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 2 , 2 , 5 ] weights = np . array ( weights ) / sum ( weights ) def metric_songs ( x : np . array , y : np . array ) -> float : similarity = 0 similarity += np . dot ( weights [ 0 : 5 ], metric_categorical ( x [ 0 : 5 ], y [ 0 : 5 ])) similarity += np . dot ( weights [ 5 : 17 ], metric_numerical ( x [ 5 : 17 ], y [ 5 : 17 ])) similarity += weights [ 17 ] * metric_set_oriented ( x [ 17 ], y [ 17 ]) return similarity Simple example Let's calculate a simple case with 500 songs. x1 = np . array ( tracks_df . drop_duplicates ( 'id' )[ features ] . sample ( 500 )) matrix = cdist ( x1 , x1 , metric = metric_songs ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) heatmap ( matrix , ax = ax , cmap = sns . light_palette ( \"green\" )) ax . set_title ( 'Similarity Matrix' ) plt . show () Recommendation based on similarity. We will use the metric described above. The similarity between two songs will be interpreted as a probability . We could build the role track similarity but it requires much computation. So we will do a simple modification. We will calculate the metric between two songs if they are in the same playlist, for some playlist in the dataset. We expect it reduces the number of calculations! After we will have a sparser matrix and in order too add tracks to a playlist, we get n tracks that maximize the mean probability on the similarity matrix, but considering only the tracks on the playlist given. If two tracks appear in the same playlists more times, we use a correction factor. We do as below, if x is the similarity between two tracks, I want f factor that: x \\leq fx < 1 \\implies 1 \\leq f < \\frac{1}{x} I take f as a convex combination of the values. So: f = \\alpha + \\frac{1 - \\alpha}{x} \\implies fx = \\alpha x + (1 - \\alpha), where \\alpha \\in (0,1] . If \\alpha = 1 , we do not have this correction. Evaluation R-precision metric As described in their work, Chen et al. suggests a metric for playlist continuation evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model suggested correctly. A playlist as input to the model has two parts: its part on display to the model and it's hidden part. The hidden part is what the model try to predict and is called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. # Class of the model class SimilarityModel : def __init__ ( self , tracks : pd . DataFrame , playlists : pd . DataFrame ): '''Implementation of the Simmilarity Model described above. The metric used are describe in PATS article. - tracks: all the tracks in your world. - playlists: the training playlists. ''' self . tracks = tracks self . playlists = playlists # We will consider a dataframe with the unique tracks and create numerical indexes self . tracks_index = self . tracks [[ 'id' ]] . drop_duplicates ( 'id' ) . reset_index () self . playlists = self . playlists . set_index ( 'playlist_id' ) def get_similar_track ( self , tracks_similarity : np . array , n_of_songs : int ): '''We get the mean in the tracks similarity and get tracks that maximize the probability mean. - tracks_similarity: matrix with similarities. - n_of_song: the number of songs wanted to be predicted. ''' interest_tracks = tracks_similarity . mean ( axis = 0 ) . A . flatten () songs = np . argpartition ( interest_tracks , - n_of_songs )[ - n_of_songs :] return songs def _get_index ( self , tracks_ids ): indexes = self . tracks_index [ self . tracks_index . id . isin ( tracks_ids )] . index return list ( indexes ) def _get_track_number ( self , index ): track_id = self . tracks_index . loc [ index ] return track_id . id def accuracy_metric ( self , predicted , true ): G_a = set () for artist_id in predicted . artists_ids : G_a = G_a . union ( artist_id ) S_a = set () for artist_id in true . artists_ids : S_a = S_a . union ( artist_id ) G_t = set ( true . id ) S_t = set ( predicted . id ) acc = ( len ( S_t & G_t ) + 0.25 * len ( S_a & G_a )) / len ( G_t ) return acc def fit ( self , alpha = 0.5 ): '''This functions build the model with the tracks and playlists disposed. (1 - alpha) increases the similarity of two tracks if they appear in more playlists. It should be between (0, 1]. ''' assert alpha > 0 assert alpha <= 1 tracks_similarity = lil_matrix (( len ( self . tracks_index ), len ( self . tracks_index )), dtype = float ) for playlist_id in tqdm ( self . playlists . index ): tracks_playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] indexes = self . _get_index ( tracks_playlist . id ) dist = squareform ( pdist ( tracks_playlist , metric = metric_songs )) # M will be a mask. I will multiply it to (fx - x) M = np . heaviside ( tracks_similarity [ np . ix_ ( indexes , indexes )] . A , 0 ) M = M * (( alpha - 1 ) * tracks_similarity [ np . ix_ ( indexes , indexes )] . A + ( 1 - alpha )) M = M + dist tracks_similarity [ np . ix_ ( indexes , indexes )] = M self . tracks_similarity = tracks_similarity . tocsr () def predict ( self , given_tracks : pd . DataFrame , n_of_songs : int ): '''Given a playlist, this function complete it with n_of_songs songs''' n = len ( given_tracks ) indexes = self . _get_index ( given_tracks . id ) similarity = self . tracks_similarity [ indexes ] tracks_chosen = self . get_similar_track ( similarity , n_of_songs ) tracks_id = self . _get_track_number ( tracks_chosen ) predicted_tracks = self . tracks [ self . tracks . id . isin ( tracks_id )] . drop_duplicates ( 'id' ) return predicted_tracks def accuracy_evaluation ( self , playlists : pd . DataFrame = None , rate = 0.7 , bar_show = True ): accuracy = [] if playlists is None : playlists = self . playlists if bar_show : iterator = tqdm ( playlists . index ) else : iterator = playlists . index for playlist_id in iterator : playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] n = len ( playlist ) if n <= 5 : continue # Already known tracks j = int ( rate * n ) if j == 0 : continue playlist_not_hidden = playlist . iloc [ 0 : j ] playlist_hidden = playlist . iloc [ j :] prediction = self . predict ( playlist_not_hidden , n - j ) acc = self . accuracy_metric ( prediction , playlist_hidden ) accuracy . append ( acc ) return np . mean ( accuracy ) Testing the Results First, I will get playlist_id for train and test. I get also only the necessary features from the tracks. I dropped the duplicates cause I'm not interested in playlists with repeated tracks, given that I already know two equal songs have similarity 1. train , test = train_test_split ( playlists_df . drop_duplicates (), test_size = 0.2 , random_state = 412 ) tracks_subset = tracks_df [ features + [ 'id' , 'playlist_id' ]] Let's train the model I will validade the alpha value. It takes a long time to do all the job. Sorry, but you'll have to wait. That's the reason I do not use Cross Validation. It would be better, however. def fitting ( alpha ): print ( 'INFO - Starting with alpha: {} \\n ' . format ( alpha )) model = SimilarityModel ( tracks_subset , training ) model . fit ( alpha = alpha ) acc = model . accuracy_evaluation ( validate , bar_show = False ) evaluation [ alpha ] = acc return acc alphas = [ 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ] training , validate = train_test_split ( train , test_size = 0.2 ) validate = validate . set_index ( 'playlist_id' ) evaluation = dict ( zip ( alphas ,[ 0 , 0 , 0 , 0 , 0 ])) for alpha in alphas : _ = fitting ( alpha ) print ( 'The chosen alpha was {} ' . format ( sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ])) The chosen alpha was (1.0, 0.055581996102373604) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( evaluation . keys (), evaluation . values ()) ax . set_title ( 'Evaluation on the Validation Set' ) ax . set_ylabel ( 'R-precision' ) ax . set_xlabel ( 'alpha' ) plt . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show () So, if two tracks appear together in more thatn a playlist, we don't use this information. Fitting the model with this alpha alpha = sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ][ 0 ] model = SimilarityModel ( tracks_subset , train ) model . fit ( alpha = alpha ) Let's see in the testing ad training set I only have to set test index to playlist_id because it is only done automatically in the training set. test = test . set_index ( 'playlist_id' ) Change the rate of known songs We were considering we already knew 70% tracks of the playlist. I vary this with some values to understang the results. rates = [ 0.2 , 0.5 , 0.7 , 0.9 ] evaluation = { 'Rate' : rates , 'Train Set' : [], 'Test Set' : []} for rate in rates : train_acc = model . accuracy_evaluation ( rate = rate ) test_acc = model . accuracy_evaluation ( test , rate = rate , bar_show = False ) evaluation [ 'Train Set' ] . append ( train_acc ) evaluation [ 'Test Set' ] . append ( test_acc ) evaluation = pd . DataFrame ( evaluation , index = range ( 4 )) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . lineplot ( x = 'Rate' , y = 'Train Set' , data = evaluation , ax = ax [ 0 ]) sns . lineplot ( x = 'Rate' , y = 'Test Set' , data = evaluation , ax = ax [ 1 ], color = 'darkred' ) fig . suptitle ( 'Evaluation on the Test and Train Set' ) ax [ 0 ] . set_title ( 'Train Set' ) ax [ 1 ] . set_title ( 'Test Set' ) ax [ 0 ] . set_ylabel ( 'R-precision' ) ax [ 1 ] . set_ylabel ( 'R-precision' ) ax [ 0 ] . set_xlabel ( 'rate' ) ax [ 1 ] . set_xlabel ( 'rate' ) ax [ 0 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show () Conclusion With low rates, the algorithm performs better. Also, we have to notice that we do not use all the information due to computational cost, but it could improve the results! A well done prefilter could be good to the data, but none thought was good enough.","title":"Model based on track similarity"},{"location":"model_1/model/#model-of-tracks-similarity-matrix","text":"","title":"Model of Track's Similarity Matrix"},{"location":"model_1/model/#based-on-the-article-steffen-pauws-and-berry-eggen","text":"The main idea is to establish a metric and build a similarity matrix indicating the probabilitity thet two songs are the same. This will be done using the track's features, track's metadata and the playlists built by the users on Spotify. # Importing libraries import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from scipy.spatial.distance import cdist , squareform , pdist from scipy.sparse import csr_matrix , lil_matrix from sklearn.model_selection import train_test_split from seaborn import heatmap import seaborn as sns import matplotlib.pyplot as plt from tqdm.notebook import tqdm import glob import os","title":"Based on the article Steffen Pauws and Berry Eggen"},{"location":"model_1/model/#defining-important-features","text":"This features will be used to understand the data. We separate the metadata and audio features needed in the process. These features are obtained from Spotify API metadata = [ 'playlist_id' , 'explicit' , 'id' , 'popularity' , 'album_id' , 'album_release_date' , 'artists_ids' ] audio_features = [ 'danceability' , 'energy' , 'loudness' , 'key' , 'mode' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'duration_ms' , 'id' ]","title":"Defining Important Features"},{"location":"model_1/model/#playlist-and-tracks-dataframes","text":"Here we get the playlists, the audio features and the tracks. I separate playlists with more the 5 tracks and less than 500, due to computational problems and considering that people do not make playlists of this size. I will only consider a random part of the dataset, due to computational costs. sample = 1500 # It must be < 10000 playlists_df = pd . read_pickle ( '../../data/sp_playlists.pkl' )[[ 'owner_id' , 'id' , 'tracks' ]] seed = np . random . RandomState ( 100 ) #Reproducibility chosen_users = seed . choice ( playlists_df . owner_id . unique (), size = sample , replace = False ) playlists_df = playlists_df [ playlists_df . owner_id . isin ( chosen_users )] playlists_df . rename ( columns = { 'id' : 'playlist_id' , 'tracks' : 'n_tracks' }, inplace = True ) playlists_df . n_tracks = playlists_df . n_tracks . apply ( lambda x : x [ 'total' ]) # Getting Playlists with at least 5 tracks and maximum of 500 tracks playlists_df = playlists_df [( playlists_df . n_tracks >= 5 ) & ( playlists_df . n_tracks <= 500 )] del playlists_df [ 'n_tracks' ] del playlists_df [ 'owner_id' ] audio_features_df = pd . read_pickle ( '../../data/sp_audio_features.pkl' )[ audio_features ] tracks_df = pd . DataFrame () for file in tqdm ( glob . glob ( '../../data/sp_tracks_ready_*.pkl' )): a = pd . read_pickle ( file )[ metadata ] a = a [ a . playlist_id . isin ( playlists_df . playlist_id )] tracks_df = pd . concat ([ tracks_df , a ], ignore_index = True ) tracks_df = tracks_df . merge ( audio_features_df , on = 'id' ) del audio_features_df del a I will disconsider duplicated songs in the same playlist. I know it may happen, but the algorithm calculates similarity between tracks, and I know it's one. tracks_df = tracks_df . drop_duplicates ([ 'id' , 'playlist_id' ])","title":"Playlist and Tracks dataframes"},{"location":"model_1/model/#treating-the-data","text":"I convert the dates to datetime and use the year as a continuum value. tracks_df [ 'album_release_date' ] . replace ( to_replace = '0000' , value = None , inplace = True ) tracks_df [ 'album_release_date' ] = pd . to_datetime ( tracks_df [ 'album_release_date' ], errors = 'coerce' ) tracks_df [ 'album_release_date' ] = ( tracks_df [ 'album_release_date' ] - tracks_df [ 'album_release_date' ] . min ()) tracks_df [ 'days' ] = tracks_df [ 'album_release_date' ] / np . timedelta64 ( 1 , 'D' ) We have some few nan values in the column days . I will put the mean of the values, because it's few missing data. It will depend on the initial sample. tracks_df [ 'days' ] . fillna ( np . mean ( tracks_df [ 'days' ]), inplace = True ) Convert the artists to set, in order to the metric presented below. It helps the analysis. tracks_df . artists_ids = tracks_df . artists_ids . apply ( set ) I separate the categorical, numerical and set oriented features, to make the ideia of the similarity matrix. This ideia is withdrawn from article already cited. features_categorical = [ 'explicit' , 'album_id' , 'key' , 'mode' , 'time_signature' ] features_numerical = [ 'popularity' , 'duration_ms' , 'danceability' , 'energy' , 'loudness' , 'speechiness' , 'acousticness' , 'instrumentalness' , 'liveness' , 'valence' , 'tempo' , 'days' ] features_set_oriented = [ 'artists_ids' ] features = [] features . extend ( features_categorical ) features . extend ( features_numerical ) features . extend ( features_set_oriented ) Only to ensure correct type of numerical features. tracks_df [ features_numerical ] = tracks_df [ features_numerical ] . astype ( float )","title":"Treating the data"},{"location":"model_1/model/#defining-the-metric","text":"Let's build the metrics proposed. For now, I normalize the numerical data, ensuring the range to be [0,1] . This ensures the metric works. Consider w a weight vector with size K , the number of features and ||w||_1 = 1 . Consider we have r, s and t features categorical, numerical and set_oriented, respectively, where r + s + t = K . Let x e y be two tracks. So: S = \\sum_{i=1}^r w_i(\\mathbb{1}\\{x_i = y_i\\}) + \\sum_{j = 1}^s w_{r + j}(1 - ||x_j - y_j||_1) + \\sum_{k=1}^t w_{r + s + k} \\frac{|x_k \\cap y_k|}{|x_k \\cup y_k|} scaler = MinMaxScaler () tracks_df [ features_numerical ] = scaler . fit_transform ( tracks_df [ features_numerical ]) I will give grades of importance (1 - 5) based on my experience to each feature. This can be changed, however it will change how the model give importance for each feature in simmilarity calculation. metric_categorical = lambda x1 , x2 : x1 == x2 metric_set_oriented = lambda x1 , x2 : len ( x1 & x2 ) / ( len ( x1 . union ( x2 ))) metric_numerical = lambda x1 , x2 : 1 - abs ( x1 - x2 ) weights = [ 1 , 5 , 2 , 3 , 3 , 2 , 3 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 4 , 2 , 2 , 5 ] weights = np . array ( weights ) / sum ( weights ) def metric_songs ( x : np . array , y : np . array ) -> float : similarity = 0 similarity += np . dot ( weights [ 0 : 5 ], metric_categorical ( x [ 0 : 5 ], y [ 0 : 5 ])) similarity += np . dot ( weights [ 5 : 17 ], metric_numerical ( x [ 5 : 17 ], y [ 5 : 17 ])) similarity += weights [ 17 ] * metric_set_oriented ( x [ 17 ], y [ 17 ]) return similarity","title":"Defining the metric"},{"location":"model_1/model/#simple-example","text":"Let's calculate a simple case with 500 songs. x1 = np . array ( tracks_df . drop_duplicates ( 'id' )[ features ] . sample ( 500 )) matrix = cdist ( x1 , x1 , metric = metric_songs ) fig , ax = plt . subplots ( figsize = ( 10 , 7 )) heatmap ( matrix , ax = ax , cmap = sns . light_palette ( \"green\" )) ax . set_title ( 'Similarity Matrix' ) plt . show ()","title":"Simple example"},{"location":"model_1/model/#recommendation-based-on-similarity","text":"We will use the metric described above. The similarity between two songs will be interpreted as a probability . We could build the role track similarity but it requires much computation. So we will do a simple modification. We will calculate the metric between two songs if they are in the same playlist, for some playlist in the dataset. We expect it reduces the number of calculations! After we will have a sparser matrix and in order too add tracks to a playlist, we get n tracks that maximize the mean probability on the similarity matrix, but considering only the tracks on the playlist given. If two tracks appear in the same playlists more times, we use a correction factor. We do as below, if x is the similarity between two tracks, I want f factor that: x \\leq fx < 1 \\implies 1 \\leq f < \\frac{1}{x} I take f as a convex combination of the values. So: f = \\alpha + \\frac{1 - \\alpha}{x} \\implies fx = \\alpha x + (1 - \\alpha), where \\alpha \\in (0,1] . If \\alpha = 1 , we do not have this correction.","title":"Recommendation based on similarity."},{"location":"model_1/model/#evaluation","text":"","title":"Evaluation"},{"location":"model_1/model/#r-precision-metric","text":"As described in their work, Chen et al. suggests a metric for playlist continuation evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model suggested correctly. A playlist as input to the model has two parts: its part on display to the model and it's hidden part. The hidden part is what the model try to predict and is called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. # Class of the model class SimilarityModel : def __init__ ( self , tracks : pd . DataFrame , playlists : pd . DataFrame ): '''Implementation of the Simmilarity Model described above. The metric used are describe in PATS article. - tracks: all the tracks in your world. - playlists: the training playlists. ''' self . tracks = tracks self . playlists = playlists # We will consider a dataframe with the unique tracks and create numerical indexes self . tracks_index = self . tracks [[ 'id' ]] . drop_duplicates ( 'id' ) . reset_index () self . playlists = self . playlists . set_index ( 'playlist_id' ) def get_similar_track ( self , tracks_similarity : np . array , n_of_songs : int ): '''We get the mean in the tracks similarity and get tracks that maximize the probability mean. - tracks_similarity: matrix with similarities. - n_of_song: the number of songs wanted to be predicted. ''' interest_tracks = tracks_similarity . mean ( axis = 0 ) . A . flatten () songs = np . argpartition ( interest_tracks , - n_of_songs )[ - n_of_songs :] return songs def _get_index ( self , tracks_ids ): indexes = self . tracks_index [ self . tracks_index . id . isin ( tracks_ids )] . index return list ( indexes ) def _get_track_number ( self , index ): track_id = self . tracks_index . loc [ index ] return track_id . id def accuracy_metric ( self , predicted , true ): G_a = set () for artist_id in predicted . artists_ids : G_a = G_a . union ( artist_id ) S_a = set () for artist_id in true . artists_ids : S_a = S_a . union ( artist_id ) G_t = set ( true . id ) S_t = set ( predicted . id ) acc = ( len ( S_t & G_t ) + 0.25 * len ( S_a & G_a )) / len ( G_t ) return acc def fit ( self , alpha = 0.5 ): '''This functions build the model with the tracks and playlists disposed. (1 - alpha) increases the similarity of two tracks if they appear in more playlists. It should be between (0, 1]. ''' assert alpha > 0 assert alpha <= 1 tracks_similarity = lil_matrix (( len ( self . tracks_index ), len ( self . tracks_index )), dtype = float ) for playlist_id in tqdm ( self . playlists . index ): tracks_playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] indexes = self . _get_index ( tracks_playlist . id ) dist = squareform ( pdist ( tracks_playlist , metric = metric_songs )) # M will be a mask. I will multiply it to (fx - x) M = np . heaviside ( tracks_similarity [ np . ix_ ( indexes , indexes )] . A , 0 ) M = M * (( alpha - 1 ) * tracks_similarity [ np . ix_ ( indexes , indexes )] . A + ( 1 - alpha )) M = M + dist tracks_similarity [ np . ix_ ( indexes , indexes )] = M self . tracks_similarity = tracks_similarity . tocsr () def predict ( self , given_tracks : pd . DataFrame , n_of_songs : int ): '''Given a playlist, this function complete it with n_of_songs songs''' n = len ( given_tracks ) indexes = self . _get_index ( given_tracks . id ) similarity = self . tracks_similarity [ indexes ] tracks_chosen = self . get_similar_track ( similarity , n_of_songs ) tracks_id = self . _get_track_number ( tracks_chosen ) predicted_tracks = self . tracks [ self . tracks . id . isin ( tracks_id )] . drop_duplicates ( 'id' ) return predicted_tracks def accuracy_evaluation ( self , playlists : pd . DataFrame = None , rate = 0.7 , bar_show = True ): accuracy = [] if playlists is None : playlists = self . playlists if bar_show : iterator = tqdm ( playlists . index ) else : iterator = playlists . index for playlist_id in iterator : playlist = self . tracks [ self . tracks . playlist_id == playlist_id ] n = len ( playlist ) if n <= 5 : continue # Already known tracks j = int ( rate * n ) if j == 0 : continue playlist_not_hidden = playlist . iloc [ 0 : j ] playlist_hidden = playlist . iloc [ j :] prediction = self . predict ( playlist_not_hidden , n - j ) acc = self . accuracy_metric ( prediction , playlist_hidden ) accuracy . append ( acc ) return np . mean ( accuracy )","title":"R-precision metric"},{"location":"model_1/model/#testing-the-results","text":"First, I will get playlist_id for train and test. I get also only the necessary features from the tracks. I dropped the duplicates cause I'm not interested in playlists with repeated tracks, given that I already know two equal songs have similarity 1. train , test = train_test_split ( playlists_df . drop_duplicates (), test_size = 0.2 , random_state = 412 ) tracks_subset = tracks_df [ features + [ 'id' , 'playlist_id' ]]","title":"Testing the Results"},{"location":"model_1/model/#lets-train-the-model","text":"I will validade the alpha value. It takes a long time to do all the job. Sorry, but you'll have to wait. That's the reason I do not use Cross Validation. It would be better, however. def fitting ( alpha ): print ( 'INFO - Starting with alpha: {} \\n ' . format ( alpha )) model = SimilarityModel ( tracks_subset , training ) model . fit ( alpha = alpha ) acc = model . accuracy_evaluation ( validate , bar_show = False ) evaluation [ alpha ] = acc return acc alphas = [ 0.2 , 0.4 , 0.6 , 0.8 , 1.0 ] training , validate = train_test_split ( train , test_size = 0.2 ) validate = validate . set_index ( 'playlist_id' ) evaluation = dict ( zip ( alphas ,[ 0 , 0 , 0 , 0 , 0 ])) for alpha in alphas : _ = fitting ( alpha ) print ( 'The chosen alpha was {} ' . format ( sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ])) The chosen alpha was (1.0, 0.055581996102373604) fig , ax = plt . subplots ( figsize = ( 10 , 5 )) ax . plot ( evaluation . keys (), evaluation . values ()) ax . set_title ( 'Evaluation on the Validation Set' ) ax . set_ylabel ( 'R-precision' ) ax . set_xlabel ( 'alpha' ) plt . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show () So, if two tracks appear together in more thatn a playlist, we don't use this information. Fitting the model with this alpha alpha = sorted ( evaluation . items (), key = lambda x : x [ 1 ], reverse = True )[ 0 ][ 0 ] model = SimilarityModel ( tracks_subset , train ) model . fit ( alpha = alpha )","title":"Let's train the model"},{"location":"model_1/model/#lets-see-in-the-testing-ad-training-set","text":"I only have to set test index to playlist_id because it is only done automatically in the training set. test = test . set_index ( 'playlist_id' )","title":"Let's see in the testing ad training set"},{"location":"model_1/model/#change-the-rate-of-known-songs","text":"We were considering we already knew 70% tracks of the playlist. I vary this with some values to understang the results. rates = [ 0.2 , 0.5 , 0.7 , 0.9 ] evaluation = { 'Rate' : rates , 'Train Set' : [], 'Test Set' : []} for rate in rates : train_acc = model . accuracy_evaluation ( rate = rate ) test_acc = model . accuracy_evaluation ( test , rate = rate , bar_show = False ) evaluation [ 'Train Set' ] . append ( train_acc ) evaluation [ 'Test Set' ] . append ( test_acc ) evaluation = pd . DataFrame ( evaluation , index = range ( 4 )) fig , ax = plt . subplots ( 1 , 2 , figsize = ( 15 , 5 )) sns . lineplot ( x = 'Rate' , y = 'Train Set' , data = evaluation , ax = ax [ 0 ]) sns . lineplot ( x = 'Rate' , y = 'Test Set' , data = evaluation , ax = ax [ 1 ], color = 'darkred' ) fig . suptitle ( 'Evaluation on the Test and Train Set' ) ax [ 0 ] . set_title ( 'Train Set' ) ax [ 1 ] . set_title ( 'Test Set' ) ax [ 0 ] . set_ylabel ( 'R-precision' ) ax [ 1 ] . set_ylabel ( 'R-precision' ) ax [ 0 ] . set_xlabel ( 'rate' ) ax [ 1 ] . set_xlabel ( 'rate' ) ax [ 0 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) ax [ 1 ] . grid ( alpha = 0.5 , color = 'grey' , linestyle = '--' ) plt . show ()","title":"Change the rate of known songs"},{"location":"model_1/model/#conclusion","text":"With low rates, the algorithm performs better. Also, we have to notice that we do not use all the information due to computational cost, but it could improve the results! A well done prefilter could be good to the data, but none thought was good enough.","title":"Conclusion"},{"location":"model_2/model/","text":"Model based on playlist similarity Given a playlist, we want to add more tracks to it: it's the playlist continuation problem. Following Kelen et al. , the idea here is to define a similarity metric between two playlists, select the k most similar playlists to ours, define a score metric for tracks continuing our playlist and choose the best tracks to continue it. from scipy.sparse import lil_matrix from tqdm.notebook import tqdm import glob import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns sns . set () from spotipy.oauth2 import SpotifyClientCredentials import spotipy auth_manager = SpotifyClientCredentials () sp = spotipy . Spotify ( auth_manager = auth_manager ) Treatment of data Tracks Here we load and treat the tracks dataset. def tracks_dfs (): \"\"\"Generator to concatenate the various files.\"\"\" for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' ): df = pd . read_pickle ( file )[[ 'id' , 'playlist_id' , 'artists_ids' ]] yield pd . concat ([ df , pd . DataFrame ({ 'file' : [ file ] * len ( df )})], axis = 1 ) tracks_df = pd . concat ( tqdm ( tracks_dfs (), total = 128 ), ignore_index = True ) tracks_df . dropna ( inplace = True ) # The following is necessary to discard repeated playlists tracks_df [ 'idx' ] = tracks_df . index grouped = tracks_df . groupby ( [ 'playlist_id' , 'file' ] )[ 'idx' ] . apply ( list ) . reset_index () tracks_df = tracks_df . drop ( index = [ el for list in grouped [ grouped . duplicated ( 'playlist_id' )] . idx for el in list ] ) del grouped tracks_df . drop ( columns = 'idx' , inplace = True ) We treat the playlists dataset: playlists = tracks_df . groupby ( 'playlist_id' )[ 'id' ] . apply ( list ) We treat the artists for each track: artists = tracks_df . drop_duplicates ( 'id' ) . set_index ( 'id' ) . artists_ids artists . index . name = 'track_id' artists_ids = dict ( zip ( artists . index , range ( len ( artists )))) Training, validation and test data Now we split the data: def split_data ( playlists , test_frac = 0.2 ): # Only playlists between 5 and 250 tracks query = playlists . apply ( lambda x : len ( x )) playlists = playlists [( query >= 5 ) & ( query <= 250 )] # Split training and test data n_test = int ( np . ceil ( len ( playlists ) * test_frac )) query = playlists . apply ( lambda x : len ( x )) playlists_test = playlists [( query > 25 )] . sample ( n_test ) playlists_training = playlists . drop ( index = playlists_test . index ) return playlists_training , playlists_test playlists_training , playlists_test = split_data ( playlists ) playlists_training , playlists_validation = split_data ( playlists_training ) The model Relevance matrix R We build the relevance matrix R . R_{ij}=r_{ij} indicates if a track j is relevant to the playlist i , that is, the track is in the playlist. Because we will use matrix multiplication, we have to index each track ID and each playlist ID to an index of the matrix. We do it here using dictionaries. def matrix_r ( playlists_training ): \"\"\"Create relevance matrix R.\"\"\" all_tracks = [] for playlist in playlists_training . to_list (): all_tracks . extend ( playlist ) all_tracks = list ( set ( all_tracks )) track_ids_go = dict ( zip ( all_tracks , range ( len ( all_tracks )))) track_ids_back = dict ( zip ( track_ids_go . values (), track_ids_go . keys ())) playlist_ids = dict ( zip ( set ( playlists_training . index ), range ( len ( set ( playlists_training . index ))) )) m = len ( set ( playlists_training . index )) n = len ( set ( all_tracks )) R = lil_matrix (( m , n )) for playlist_id , playlist in playlists_training . iteritems (): for track_id in playlist : R [ playlist_ids [ playlist_id ], track_ids_go [ track_id ]] = 1 return track_ids_go , track_ids_back , playlist_ids , R Similarity The similarity between two playlists u and v is calculated by: s_{uv} = \\sum_{i \\in I} \\dfrac{r_{ui}r_{vi}}{||R_u||_2||R_v||_2} I is the set of tracks and R_u is the vector of relevances r_{ui} for the playlist u . In fact, we basically count the number of tracks in the intersection of the playlists and normalize it. def similarity ( playlist_1 , playlist_2 ): \"\"\"Calculate the similarity between two playlists.\"\"\" summation = len ( set ( playlist_1 ) & set ( playlist_2 )) if summation == 0 : return 0 return summation / np . sqrt ( len ( playlist_1 ) * len ( playlist_2 )) Track score Given a playlist u to be continuated, we calculate the similarity of it with all existent playlists and select the k most similar playlists, that is, the set N_k(u) . So, we define a score for a track to be in the playlist: \\hat{r}_{ui} = \\dfrac{\\sum_{v \\in N_k(u)} s_{uv} \\cdot r_{vi}}{\\sum_{v \\in N_k(u)} s_{uv}} The intuition is that we are giving high scores to tracks that are in many playlists with great similarities to our playlist. We return the tracks ordered by score. def continuation ( R , playlist , playlists_training , k , playlist_ids , track_ids_back ): \"\"\"Continue a playlist based on k most similar playlists.\"\"\" m = len ( set ( playlists_training . index )) s_u = lil_matrix (( 1 , m )) for alt_playlist_index , alt_playlist in playlists_training . items (): s = similarity ( playlist , alt_playlist ) s_u [ 0 , playlist_ids [ alt_playlist_index ]] = s sorted_similarities_indices = np . flip ( np . argsort ( s_u . toarray ()[ 0 ])) top_k_similarities_indices = sorted_similarities_indices [: k ] scores = s_u [ 0 , top_k_similarities_indices ] * R [ top_k_similarities_indices , :] scores = scores . toarray ()[ 0 ] sorted_scores_indices = np . flip ( np . argsort ( scores )[ - 225 :]) return [ track_ids_back [ index ] for index in sorted_scores_indices ] Summary We choose a playlist to be continuated; We calculate the similarity between this and each playlist in the training dataset; We calculate the score of each track continuating our playlist; We choose the tracks with highest score. Evaluation R-precision metric As described in their work, Chen et al. suggest a metric for playlist continuation playlist evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model correctly suggested. A playlist as input to the model has two parts: the part the model will see and the part the model will try to predict, called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. def r_precision ( S_t , G_t , S_a , G_a ): return ( len ( set ( S_t ) & set ( G_t )) + 0.25 * len ( set ( S_a ) & set ( G_a ))) / len ( G_t ) def evaluation ( playlist_not_hidden , playlist_hidden , continuation ): for track in playlist_not_hidden : if track in continuation : continuation . remove ( track ) continuation = continuation [: len ( playlist_hidden )] G_a = [] for track in playlist_hidden : G_a . extend ( artists . iloc [ artists_ids [ track ]]) S_a = [] for track in continuation : S_a . extend ( artists . iloc [ artists_ids [ track ]]) metric = r_precision ( continuation , playlist_hidden , S_a , G_a ) return metric Hyperparameter k We now select a k value that maximizes the R-precision metric in a sample in our validation dataset. It's not feasible to select k by cross-validation, because we need test data to have more than 25 tracks. track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_training ) metrics = [] for k in tqdm ([ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )]): metric_summation = 0 for playlist in tqdm ( playlists_validation . sample ( 1000 )): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric metrics . append ( metric_summation / 1000 ) sns . lineplot ( x = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )], y = metrics , marker = 'o' ) plt . title ( 'R-precision vs. k' ) plt . xlabel ( 'k' ) plt . ylabel ( 'R-precision' ) plt . show () k = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )][ np . argmax ( metrics )] print ( 'The best k is {} .' . format ( k )) The best k is 100. Now we train and evaluate our model: playlists_all_training = pd . concat ([ playlists_training , playlists_validation ]) track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_all_training ) metric_summation = 0 for playlist in tqdm ( playlists_test ): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_all_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric print ( 'R-precision = {:.4f} ' . format ( metric_summation / len ( playlists_test ))) R-precision = 0.1107 As said by Chen et al. , the highest performance achieved in RecSys Challenge 2018 was 0.2241. Well, many competitors were using much more advanced models, like neural networks, and they also had much more data and possible more computational power. So the results we achieved seems much reasonable.","title":"Model based on playlist similarity"},{"location":"model_2/model/#model-based-on-playlist-similarity","text":"Given a playlist, we want to add more tracks to it: it's the playlist continuation problem. Following Kelen et al. , the idea here is to define a similarity metric between two playlists, select the k most similar playlists to ours, define a score metric for tracks continuing our playlist and choose the best tracks to continue it. from scipy.sparse import lil_matrix from tqdm.notebook import tqdm import glob import matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns sns . set () from spotipy.oauth2 import SpotifyClientCredentials import spotipy auth_manager = SpotifyClientCredentials () sp = spotipy . Spotify ( auth_manager = auth_manager )","title":"Model based on playlist similarity"},{"location":"model_2/model/#treatment-of-data","text":"","title":"Treatment of data"},{"location":"model_2/model/#tracks","text":"Here we load and treat the tracks dataset. def tracks_dfs (): \"\"\"Generator to concatenate the various files.\"\"\" for file in glob . glob ( '../../data/sp_tracks_ready_*.pkl' ): df = pd . read_pickle ( file )[[ 'id' , 'playlist_id' , 'artists_ids' ]] yield pd . concat ([ df , pd . DataFrame ({ 'file' : [ file ] * len ( df )})], axis = 1 ) tracks_df = pd . concat ( tqdm ( tracks_dfs (), total = 128 ), ignore_index = True ) tracks_df . dropna ( inplace = True ) # The following is necessary to discard repeated playlists tracks_df [ 'idx' ] = tracks_df . index grouped = tracks_df . groupby ( [ 'playlist_id' , 'file' ] )[ 'idx' ] . apply ( list ) . reset_index () tracks_df = tracks_df . drop ( index = [ el for list in grouped [ grouped . duplicated ( 'playlist_id' )] . idx for el in list ] ) del grouped tracks_df . drop ( columns = 'idx' , inplace = True ) We treat the playlists dataset: playlists = tracks_df . groupby ( 'playlist_id' )[ 'id' ] . apply ( list ) We treat the artists for each track: artists = tracks_df . drop_duplicates ( 'id' ) . set_index ( 'id' ) . artists_ids artists . index . name = 'track_id' artists_ids = dict ( zip ( artists . index , range ( len ( artists ))))","title":"Tracks"},{"location":"model_2/model/#training-validation-and-test-data","text":"Now we split the data: def split_data ( playlists , test_frac = 0.2 ): # Only playlists between 5 and 250 tracks query = playlists . apply ( lambda x : len ( x )) playlists = playlists [( query >= 5 ) & ( query <= 250 )] # Split training and test data n_test = int ( np . ceil ( len ( playlists ) * test_frac )) query = playlists . apply ( lambda x : len ( x )) playlists_test = playlists [( query > 25 )] . sample ( n_test ) playlists_training = playlists . drop ( index = playlists_test . index ) return playlists_training , playlists_test playlists_training , playlists_test = split_data ( playlists ) playlists_training , playlists_validation = split_data ( playlists_training )","title":"Training, validation and test data"},{"location":"model_2/model/#the-model","text":"","title":"The model"},{"location":"model_2/model/#relevance-matrix-r","text":"We build the relevance matrix R . R_{ij}=r_{ij} indicates if a track j is relevant to the playlist i , that is, the track is in the playlist. Because we will use matrix multiplication, we have to index each track ID and each playlist ID to an index of the matrix. We do it here using dictionaries. def matrix_r ( playlists_training ): \"\"\"Create relevance matrix R.\"\"\" all_tracks = [] for playlist in playlists_training . to_list (): all_tracks . extend ( playlist ) all_tracks = list ( set ( all_tracks )) track_ids_go = dict ( zip ( all_tracks , range ( len ( all_tracks )))) track_ids_back = dict ( zip ( track_ids_go . values (), track_ids_go . keys ())) playlist_ids = dict ( zip ( set ( playlists_training . index ), range ( len ( set ( playlists_training . index ))) )) m = len ( set ( playlists_training . index )) n = len ( set ( all_tracks )) R = lil_matrix (( m , n )) for playlist_id , playlist in playlists_training . iteritems (): for track_id in playlist : R [ playlist_ids [ playlist_id ], track_ids_go [ track_id ]] = 1 return track_ids_go , track_ids_back , playlist_ids , R","title":"Relevance matrix R"},{"location":"model_2/model/#similarity","text":"The similarity between two playlists u and v is calculated by: s_{uv} = \\sum_{i \\in I} \\dfrac{r_{ui}r_{vi}}{||R_u||_2||R_v||_2} I is the set of tracks and R_u is the vector of relevances r_{ui} for the playlist u . In fact, we basically count the number of tracks in the intersection of the playlists and normalize it. def similarity ( playlist_1 , playlist_2 ): \"\"\"Calculate the similarity between two playlists.\"\"\" summation = len ( set ( playlist_1 ) & set ( playlist_2 )) if summation == 0 : return 0 return summation / np . sqrt ( len ( playlist_1 ) * len ( playlist_2 ))","title":"Similarity"},{"location":"model_2/model/#track-score","text":"Given a playlist u to be continuated, we calculate the similarity of it with all existent playlists and select the k most similar playlists, that is, the set N_k(u) . So, we define a score for a track to be in the playlist: \\hat{r}_{ui} = \\dfrac{\\sum_{v \\in N_k(u)} s_{uv} \\cdot r_{vi}}{\\sum_{v \\in N_k(u)} s_{uv}} The intuition is that we are giving high scores to tracks that are in many playlists with great similarities to our playlist. We return the tracks ordered by score. def continuation ( R , playlist , playlists_training , k , playlist_ids , track_ids_back ): \"\"\"Continue a playlist based on k most similar playlists.\"\"\" m = len ( set ( playlists_training . index )) s_u = lil_matrix (( 1 , m )) for alt_playlist_index , alt_playlist in playlists_training . items (): s = similarity ( playlist , alt_playlist ) s_u [ 0 , playlist_ids [ alt_playlist_index ]] = s sorted_similarities_indices = np . flip ( np . argsort ( s_u . toarray ()[ 0 ])) top_k_similarities_indices = sorted_similarities_indices [: k ] scores = s_u [ 0 , top_k_similarities_indices ] * R [ top_k_similarities_indices , :] scores = scores . toarray ()[ 0 ] sorted_scores_indices = np . flip ( np . argsort ( scores )[ - 225 :]) return [ track_ids_back [ index ] for index in sorted_scores_indices ]","title":"Track score"},{"location":"model_2/model/#summary","text":"We choose a playlist to be continuated; We calculate the similarity between this and each playlist in the training dataset; We calculate the score of each track continuating our playlist; We choose the tracks with highest score.","title":"Summary"},{"location":"model_2/model/#evaluation","text":"","title":"Evaluation"},{"location":"model_2/model/#r-precision-metric","text":"As described in their work, Chen et al. suggest a metric for playlist continuation playlist evaluation. They call it R-precision . It measures how many of the real tracks (and their artists) the model correctly suggested. A playlist as input to the model has two parts: the part the model will see and the part the model will try to predict, called ground truth . \\textrm{R-precision} = \\dfrac{|S_T \\cap G_T| + 0.25 \\cdot |S_A \\cap G_A|}{|G_T|} G_T is the set of unique track IDs from ground truth, that is, the unique hidden tracks. S_T is the suggested tracks from our model. G_A is the set of unique artists IDs from ground truth and S_A is the set of predicted artists. The metric can be interpreted as accuracy (although it can be greater than 1), but giving some score for wrong tracks with right artists. def r_precision ( S_t , G_t , S_a , G_a ): return ( len ( set ( S_t ) & set ( G_t )) + 0.25 * len ( set ( S_a ) & set ( G_a ))) / len ( G_t ) def evaluation ( playlist_not_hidden , playlist_hidden , continuation ): for track in playlist_not_hidden : if track in continuation : continuation . remove ( track ) continuation = continuation [: len ( playlist_hidden )] G_a = [] for track in playlist_hidden : G_a . extend ( artists . iloc [ artists_ids [ track ]]) S_a = [] for track in continuation : S_a . extend ( artists . iloc [ artists_ids [ track ]]) metric = r_precision ( continuation , playlist_hidden , S_a , G_a ) return metric","title":"R-precision metric"},{"location":"model_2/model/#hyperparameter-k","text":"We now select a k value that maximizes the R-precision metric in a sample in our validation dataset. It's not feasible to select k by cross-validation, because we need test data to have more than 25 tracks. track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_training ) metrics = [] for k in tqdm ([ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )]): metric_summation = 0 for playlist in tqdm ( playlists_validation . sample ( 1000 )): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric metrics . append ( metric_summation / 1000 ) sns . lineplot ( x = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )], y = metrics , marker = 'o' ) plt . title ( 'R-precision vs. k' ) plt . xlabel ( 'k' ) plt . ylabel ( 'R-precision' ) plt . show () k = [ 1 , 10 , 100 , 4000 , 10000 , len ( playlists_training )][ np . argmax ( metrics )] print ( 'The best k is {} .' . format ( k )) The best k is 100. Now we train and evaluate our model: playlists_all_training = pd . concat ([ playlists_training , playlists_validation ]) track_ids_go , track_ids_back , playlist_ids , R = matrix_r ( playlists_all_training ) metric_summation = 0 for playlist in tqdm ( playlists_test ): playlist_not_hidden = playlist [: 25 ] playlist_hidden = playlist [ 25 :] continuated = continuation ( R , playlist_not_hidden , playlists_all_training , k , playlist_ids , track_ids_back ) metric = evaluation ( playlist_not_hidden , playlist_hidden , continuated ) metric_summation += metric print ( 'R-precision = {:.4f} ' . format ( metric_summation / len ( playlists_test ))) R-precision = 0.1107 As said by Chen et al. , the highest performance achieved in RecSys Challenge 2018 was 0.2241. Well, many competitors were using much more advanced models, like neural networks, and they also had much more data and possible more computational power. So the results we achieved seems much reasonable.","title":"Hyperparameter k"}]}